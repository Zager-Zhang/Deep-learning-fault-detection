12-23 15:12:33 model_name: Ae2d
12-23 15:12:33 data_name: CWRUCWT
12-23 15:12:33 data_dir: C:\Users\Tracy_Lucia\Desktop\CWRU
12-23 15:12:33 normlizetype: 0-1
12-23 15:12:33 processing_type: R_A
12-23 15:12:33 cuda_device: 0
12-23 15:12:33 checkpoint_dir: ./checkpoint
12-23 15:12:33 pretrained: True
12-23 15:12:33 batch_size: 32
12-23 15:12:33 num_workers: 0
12-23 15:12:33 opt: adam
12-23 15:12:33 lr: 0.001
12-23 15:12:33 momentum: 0.9
12-23 15:12:33 weight_decay: 1e-05
12-23 15:12:33 lr_scheduler: fix
12-23 15:12:33 gamma: 0.1
12-23 15:12:33 steps: 10,20,30,40
12-23 15:12:33 steps1: 50,80
12-23 15:12:33 middle_epoch: 50
12-23 15:12:33 max_epoch: 100
12-23 15:12:33 print_step: 100
12-23 15:12:33 using 1 cpu
12-23 15:12:41 -----Epoch 0/49-----
12-23 15:12:41 current lr: 0.001
12-23 15:12:41 Epoch: 0 [0/1068], Train Loss: 0.4161119.8 examples/sec 0.27 sec/batch
12-23 15:12:45 Epoch: 0 train-Loss: 0.2167, Cost 4.6690 sec
12-23 15:12:46 Epoch: 0 val-Loss: 0.0803, Cost 0.4130 sec
12-23 15:12:46 -----Epoch 1/49-----
12-23 15:12:46 current lr: 0.001
12-23 15:12:50 Epoch: 1 train-Loss: 0.1407, Cost 4.4883 sec
12-23 15:12:51 Epoch: 1 val-Loss: 0.1332, Cost 0.4065 sec
12-23 15:12:51 -----Epoch 2/49-----
12-23 15:12:51 current lr: 0.001
12-23 15:12:55 Epoch: 2 [1024/1068], Train Loss: 0.1506223.8 examples/sec 0.14 sec/batch
12-23 15:12:55 Epoch: 2 train-Loss: 0.1017, Cost 4.4704 sec
12-23 15:12:56 Epoch: 2 val-Loss: 0.0734, Cost 0.4227 sec
12-23 15:12:56 -----Epoch 3/49-----
12-23 15:12:56 current lr: 0.001
12-23 15:13:00 Epoch: 3 train-Loss: 0.0775, Cost 4.5010 sec
12-23 15:13:00 Epoch: 3 val-Loss: 0.0651, Cost 0.4334 sec
12-23 15:13:00 -----Epoch 4/49-----
12-23 15:13:00 current lr: 0.001
12-23 15:13:05 Epoch: 4 train-Loss: 0.0667, Cost 4.4984 sec
12-23 15:13:05 Epoch: 4 val-Loss: 0.0617, Cost 0.4200 sec
12-23 15:13:05 -----Epoch 5/49-----
12-23 15:13:05 current lr: 0.001
12-23 15:13:10 Epoch: 5 [960/1068], Train Loss: 0.0671216.8 examples/sec 0.14 sec/batch
12-23 15:13:10 Epoch: 5 train-Loss: 0.0557, Cost 4.4790 sec
12-23 15:13:10 Epoch: 5 val-Loss: 0.0496, Cost 0.4200 sec
12-23 15:13:10 -----Epoch 6/49-----
12-23 15:13:10 current lr: 0.001
12-23 15:13:15 Epoch: 6 train-Loss: 0.0449, Cost 4.6733 sec
12-23 15:13:15 Epoch: 6 val-Loss: 0.0356, Cost 0.4159 sec
12-23 15:13:15 -----Epoch 7/49-----
12-23 15:13:15 current lr: 0.001
12-23 15:13:32 Epoch: 7 train-Loss: 0.0382, Cost 16.9144 sec
12-23 15:13:34 Epoch: 7 val-Loss: 0.0316, Cost 1.7561 sec
12-23 15:13:34 -----Epoch 8/49-----
12-23 15:13:34 current lr: 0.001
12-23 15:13:49 Epoch: 8 [896/1068], Train Loss: 0.038780.3 examples/sec 0.39 sec/batch
12-23 15:13:51 Epoch: 8 train-Loss: 0.0314, Cost 16.8724 sec
12-23 15:13:53 Epoch: 8 val-Loss: 0.0357, Cost 1.8847 sec
12-23 15:13:53 -----Epoch 9/49-----
12-23 15:13:53 current lr: 0.001
12-23 15:14:10 Epoch: 9 train-Loss: 0.0278, Cost 16.8951 sec
12-23 15:14:12 Epoch: 9 val-Loss: 0.0231, Cost 1.8671 sec
12-23 15:14:12 -----Epoch 10/49-----
12-23 15:14:12 current lr: 0.001
12-23 15:14:30 Epoch: 10 train-Loss: 0.0254, Cost 17.9828 sec
12-23 15:14:32 Epoch: 10 val-Loss: 0.0215, Cost 1.9711 sec
12-23 15:14:32 -----Epoch 11/49-----
12-23 15:14:32 current lr: 0.001
12-23 15:14:46 Epoch: 11 [832/1068], Train Loss: 0.026254.9 examples/sec 0.57 sec/batch
12-23 15:14:49 Epoch: 11 train-Loss: 0.0247, Cost 17.5761 sec
12-23 15:14:51 Epoch: 11 val-Loss: 0.0197, Cost 1.8685 sec
12-23 15:14:51 -----Epoch 12/49-----
12-23 15:14:51 current lr: 0.001
12-23 15:15:08 Epoch: 12 train-Loss: 0.0218, Cost 17.4408 sec
12-23 15:15:10 Epoch: 12 val-Loss: 0.0237, Cost 1.9032 sec
12-23 15:15:10 -----Epoch 13/49-----
12-23 15:15:10 current lr: 0.001
12-23 15:15:28 Epoch: 13 train-Loss: 0.0194, Cost 18.1187 sec
12-23 15:15:30 Epoch: 13 val-Loss: 0.0193, Cost 1.9646 sec
12-23 15:15:30 -----Epoch 14/49-----
12-23 15:15:30 current lr: 0.001
12-23 15:15:44 Epoch: 14 [768/1068], Train Loss: 0.020054.0 examples/sec 0.58 sec/batch
12-23 15:15:49 Epoch: 14 train-Loss: 0.0163, Cost 18.1671 sec
12-23 15:15:50 Epoch: 14 val-Loss: 0.0094, Cost 1.8958 sec
12-23 15:15:50 -----Epoch 15/49-----
12-23 15:15:50 current lr: 0.001
12-23 15:16:09 Epoch: 15 train-Loss: 0.0141, Cost 18.1584 sec
12-23 15:16:11 Epoch: 15 val-Loss: 0.0118, Cost 1.8831 sec
12-23 15:16:11 -----Epoch 16/49-----
12-23 15:16:11 current lr: 0.001
12-23 15:16:29 Epoch: 16 train-Loss: 0.0126, Cost 18.2265 sec
12-23 15:16:31 Epoch: 16 val-Loss: 0.0116, Cost 1.9723 sec
12-23 15:16:31 -----Epoch 17/49-----
12-23 15:16:31 current lr: 0.001
12-23 15:16:43 Epoch: 17 [704/1068], Train Loss: 0.013453.1 examples/sec 0.59 sec/batch
12-23 15:16:49 Epoch: 17 train-Loss: 0.0140, Cost 18.0723 sec
12-23 15:16:51 Epoch: 17 val-Loss: 0.0091, Cost 1.9310 sec
12-23 15:16:51 -----Epoch 18/49-----
12-23 15:16:51 current lr: 0.001
12-23 15:17:09 Epoch: 18 train-Loss: 0.0115, Cost 18.1309 sec
12-23 15:17:11 Epoch: 18 val-Loss: 0.0061, Cost 1.9377 sec
12-23 15:17:11 -----Epoch 19/49-----
12-23 15:17:11 current lr: 0.001
12-23 15:17:25 Epoch: 19 train-Loss: 0.0099, Cost 14.0322 sec
12-23 15:17:25 Epoch: 19 val-Loss: 0.0072, Cost 0.4495 sec
12-23 15:17:25 -----Epoch 20/49-----
12-23 15:17:25 current lr: 0.001
12-23 15:17:28 Epoch: 20 [640/1068], Train Loss: 0.011369.4 examples/sec 0.45 sec/batch
12-23 15:17:30 Epoch: 20 train-Loss: 0.0104, Cost 4.8249 sec
12-23 15:17:31 Epoch: 20 val-Loss: 0.0068, Cost 0.4054 sec
12-23 15:17:31 -----Epoch 21/49-----
12-23 15:17:31 current lr: 0.001
12-23 15:17:35 Epoch: 21 train-Loss: 0.0101, Cost 4.4176 sec
12-23 15:17:35 Epoch: 21 val-Loss: 0.0061, Cost 0.4183 sec
12-23 15:17:35 -----Epoch 22/49-----
12-23 15:17:35 current lr: 0.001
12-23 15:17:41 Epoch: 22 train-Loss: 0.0090, Cost 5.8221 sec
12-23 15:17:42 Epoch: 22 val-Loss: 0.0073, Cost 0.6966 sec
12-23 15:17:42 -----Epoch 23/49-----
12-23 15:17:42 current lr: 0.001
12-23 15:17:47 Epoch: 23 [576/1068], Train Loss: 0.0096172.6 examples/sec 0.18 sec/batch
12-23 15:17:50 Epoch: 23 train-Loss: 0.0097, Cost 8.0437 sec
12-23 15:17:50 Epoch: 23 val-Loss: 0.0085, Cost 0.4474 sec
12-23 15:17:50 -----Epoch 24/49-----
12-23 15:17:50 current lr: 0.001
12-23 15:17:56 Epoch: 24 train-Loss: 0.0083, Cost 5.2468 sec
12-23 15:17:56 Epoch: 24 val-Loss: 0.0063, Cost 0.4253 sec
12-23 15:17:56 -----Epoch 25/49-----
12-23 15:17:56 current lr: 0.001
12-23 15:18:09 Epoch: 25 train-Loss: 0.0091, Cost 12.4569 sec
12-23 15:18:09 Epoch: 25 val-Loss: 0.0060, Cost 0.4488 sec
12-23 15:18:09 -----Epoch 26/49-----
12-23 15:18:09 current lr: 0.001
12-23 15:18:11 Epoch: 26 [512/1068], Train Loss: 0.0088127.5 examples/sec 0.25 sec/batch
12-23 15:18:13 Epoch: 26 train-Loss: 0.0080, Cost 4.3365 sec
12-23 15:18:14 Epoch: 26 val-Loss: 0.0080, Cost 0.4143 sec
12-23 15:18:14 -----Epoch 27/49-----
12-23 15:18:14 current lr: 0.001
12-23 15:18:24 Epoch: 27 train-Loss: 0.0090, Cost 10.5346 sec
12-23 15:18:26 Epoch: 27 val-Loss: 0.0054, Cost 1.9146 sec
12-23 15:18:26 -----Epoch 28/49-----
12-23 15:18:26 current lr: 0.001
12-23 15:18:43 Epoch: 28 train-Loss: 0.0074, Cost 16.8359 sec
12-23 15:18:45 Epoch: 28 val-Loss: 0.0045, Cost 1.5495 sec
12-23 15:18:45 -----Epoch 29/49-----
12-23 15:18:45 current lr: 0.001
12-23 15:18:52 Epoch: 29 [448/1068], Train Loss: 0.008076.4 examples/sec 0.41 sec/batch
12-23 15:19:02 Epoch: 29 train-Loss: 0.0078, Cost 17.2317 sec
12-23 15:19:04 Epoch: 29 val-Loss: 0.0057, Cost 1.8408 sec
12-23 15:19:04 -----Epoch 30/49-----
12-23 15:19:04 current lr: 0.001
12-23 15:19:20 Epoch: 30 train-Loss: 0.0069, Cost 16.8815 sec
12-23 15:19:22 Epoch: 30 val-Loss: 0.0043, Cost 1.9334 sec
12-23 15:19:22 -----Epoch 31/49-----
12-23 15:19:22 current lr: 0.001
12-23 15:19:39 Epoch: 31 train-Loss: 0.0063, Cost 16.7882 sec
12-23 15:19:41 Epoch: 31 val-Loss: 0.0040, Cost 1.8720 sec
12-23 15:19:41 -----Epoch 32/49-----
12-23 15:19:41 current lr: 0.001
12-23 15:19:43 Epoch: 32 [384/1068], Train Loss: 0.007062.2 examples/sec 0.50 sec/batch
12-23 15:19:45 Epoch: 32 train-Loss: 0.0075, Cost 4.3491 sec
12-23 15:19:46 Epoch: 32 val-Loss: 0.0044, Cost 0.4121 sec
12-23 15:19:46 -----Epoch 33/49-----
12-23 15:19:46 current lr: 0.001
12-23 15:19:50 Epoch: 33 train-Loss: 0.0066, Cost 4.3429 sec
12-23 15:19:51 Epoch: 33 val-Loss: 0.0058, Cost 0.4113 sec
12-23 15:19:51 -----Epoch 34/49-----
12-23 15:19:51 current lr: 0.001
12-23 15:19:58 Epoch: 34 train-Loss: 0.0077, Cost 7.4603 sec
12-23 15:20:00 Epoch: 34 val-Loss: 0.0044, Cost 1.8900 sec
12-23 15:20:00 -----Epoch 35/49-----
12-23 15:20:00 current lr: 0.001
12-23 15:20:05 Epoch: 35 [320/1068], Train Loss: 0.0073138.2 examples/sec 0.23 sec/batch
12-23 15:20:17 Epoch: 35 train-Loss: 0.0070, Cost 17.3946 sec
12-23 15:20:19 Epoch: 35 val-Loss: 0.0043, Cost 1.9064 sec
12-23 15:20:19 -----Epoch 36/49-----
12-23 15:20:19 current lr: 0.001
12-23 15:20:36 Epoch: 36 train-Loss: 0.0066, Cost 16.9377 sec
12-23 15:20:38 Epoch: 36 val-Loss: 0.0039, Cost 1.8350 sec
12-23 15:20:38 -----Epoch 37/49-----
12-23 15:20:38 current lr: 0.001
12-23 15:20:55 Epoch: 37 train-Loss: 0.0065, Cost 17.4046 sec
12-23 15:20:57 Epoch: 37 val-Loss: 0.0042, Cost 1.9162 sec
12-23 15:20:57 -----Epoch 38/49-----
12-23 15:20:57 current lr: 0.001
12-23 15:21:02 Epoch: 38 [256/1068], Train Loss: 0.006755.6 examples/sec 0.56 sec/batch
12-23 15:21:15 Epoch: 38 train-Loss: 0.0066, Cost 17.6014 sec
12-23 15:21:17 Epoch: 38 val-Loss: 0.0044, Cost 1.8798 sec
12-23 15:21:17 -----Epoch 39/49-----
12-23 15:21:17 current lr: 0.001
12-23 15:21:35 Epoch: 39 train-Loss: 0.0057, Cost 17.7911 sec
12-23 15:21:37 Epoch: 39 val-Loss: 0.0042, Cost 1.8848 sec
12-23 15:21:37 -----Epoch 40/49-----
12-23 15:21:37 current lr: 0.001
12-23 15:21:54 Epoch: 40 train-Loss: 0.0056, Cost 17.6526 sec
12-23 15:21:56 Epoch: 40 val-Loss: 0.0036, Cost 1.9637 sec
12-23 15:21:56 -----Epoch 41/49-----
12-23 15:21:56 current lr: 0.001
12-23 15:22:00 Epoch: 41 [192/1068], Train Loss: 0.005954.1 examples/sec 0.58 sec/batch
12-23 15:22:14 Epoch: 41 train-Loss: 0.0057, Cost 18.0453 sec
12-23 15:22:16 Epoch: 41 val-Loss: 0.0039, Cost 1.8523 sec
12-23 15:22:16 -----Epoch 42/49-----
12-23 15:22:16 current lr: 0.001
12-23 15:22:34 Epoch: 42 train-Loss: 0.0062, Cost 18.1140 sec
12-23 15:22:36 Epoch: 42 val-Loss: 0.0039, Cost 1.9777 sec
12-23 15:22:36 -----Epoch 43/49-----
12-23 15:22:36 current lr: 0.001
12-23 15:22:54 Epoch: 43 train-Loss: 0.0062, Cost 18.1712 sec
12-23 15:22:56 Epoch: 43 val-Loss: 0.0035, Cost 1.9582 sec
12-23 15:22:56 -----Epoch 44/49-----
12-23 15:22:56 current lr: 0.001
12-23 15:22:59 Epoch: 44 [128/1068], Train Loss: 0.006053.2 examples/sec 0.59 sec/batch
12-23 15:23:16 Epoch: 44 train-Loss: 0.0061, Cost 19.2580 sec
12-23 15:23:18 Epoch: 44 val-Loss: 0.0032, Cost 2.2003 sec
12-23 15:23:18 -----Epoch 45/49-----
12-23 15:23:18 current lr: 0.001
12-23 15:23:37 Epoch: 45 train-Loss: 0.0053, Cost 19.6472 sec
12-23 15:23:40 Epoch: 45 val-Loss: 0.0034, Cost 2.2489 sec
12-23 15:23:40 -----Epoch 46/49-----
12-23 15:23:40 current lr: 0.001
12-23 15:24:00 Epoch: 46 train-Loss: 0.0050, Cost 20.3152 sec
12-23 15:24:02 Epoch: 46 val-Loss: 0.0034, Cost 2.3061 sec
12-23 15:24:02 -----Epoch 47/49-----
12-23 15:24:02 current lr: 0.001
12-23 15:24:04 Epoch: 47 [64/1068], Train Loss: 0.005548.3 examples/sec 0.65 sec/batch
12-23 15:24:22 Epoch: 47 train-Loss: 0.0047, Cost 20.2350 sec
12-23 15:24:25 Epoch: 47 val-Loss: 0.0031, Cost 2.2641 sec
12-23 15:24:25 -----Epoch 48/49-----
12-23 15:24:25 current lr: 0.001
12-23 15:24:44 Epoch: 48 train-Loss: 0.0062, Cost 19.4784 sec
12-23 15:24:46 Epoch: 48 val-Loss: 0.0035, Cost 2.1621 sec
12-23 15:24:46 -----Epoch 49/49-----
12-23 15:24:46 current lr: 0.001
12-23 15:25:06 Epoch: 49 train-Loss: 0.0053, Cost 20.0662 sec
12-23 15:25:09 Epoch: 49 val-Loss: 0.0030, Cost 2.2678 sec
12-23 15:25:09 -----Epoch 0/99-----
12-23 15:25:09 current lr: 0.001
12-23 15:25:09 Epoch: 0 [0/1068], Train Loss: 0.0566 Train Acc: 0.0019,48.3 examples/sec 0.65 sec/batch
12-23 15:25:15 Epoch: 0 train-Loss: 2.5144 train-Acc: 0.1891, Cost 6.3687 sec
12-23 15:25:16 Epoch: 0 val-Loss: 1.9949 val-Acc: 0.2996, Cost 0.6941 sec
12-23 15:25:16 save best model epoch 0, acc 0.2996
12-23 15:25:16 -----Epoch 1/99-----
12-23 15:25:16 current lr: 0.001
12-23 15:25:19 Epoch: 1 train-Loss: 1.8048 train-Acc: 0.3652, Cost 3.6454 sec
12-23 15:25:20 Epoch: 1 val-Loss: 1.6825 val-Acc: 0.4195, Cost 0.1630 sec
12-23 15:25:20 save best model epoch 1, acc 0.4195
12-23 15:25:20 -----Epoch 2/99-----
12-23 15:25:20 current lr: 0.001
12-23 15:25:21 Epoch: 2 [1024/1068], Train Loss: 1.9204 Train Acc: 0.3354,261.5 examples/sec 0.12 sec/batch
12-23 15:25:21 Epoch: 2 train-Loss: 1.5311 train-Acc: 0.4513, Cost 1.5332 sec
12-23 15:25:21 Epoch: 2 val-Loss: 1.4411 val-Acc: 0.5056, Cost 0.1643 sec
12-23 15:25:21 save best model epoch 2, acc 0.5056
12-23 15:25:21 -----Epoch 3/99-----
12-23 15:25:21 current lr: 0.001
12-23 15:25:23 Epoch: 3 train-Loss: 1.3923 train-Acc: 0.5028, Cost 1.5094 sec
12-23 15:25:23 Epoch: 3 val-Loss: 1.2952 val-Acc: 0.5206, Cost 0.1621 sec
12-23 15:25:23 save best model epoch 3, acc 0.5206
12-23 15:25:23 -----Epoch 4/99-----
12-23 15:25:23 current lr: 0.001
12-23 15:25:25 Epoch: 4 train-Loss: 1.2333 train-Acc: 0.5646, Cost 1.5850 sec
12-23 15:25:25 Epoch: 4 val-Loss: 1.2132 val-Acc: 0.5243, Cost 0.1899 sec
12-23 15:25:25 save best model epoch 4, acc 0.5243
12-23 15:25:25 -----Epoch 5/99-----
12-23 15:25:25 current lr: 0.001
12-23 15:25:26 Epoch: 5 [960/1068], Train Loss: 1.2456 Train Acc: 0.5548,620.4 examples/sec 0.05 sec/batch
12-23 15:25:26 Epoch: 5 train-Loss: 1.0874 train-Acc: 0.6002, Cost 1.5360 sec
12-23 15:25:26 Epoch: 5 val-Loss: 1.0485 val-Acc: 0.6142, Cost 0.1658 sec
12-23 15:25:26 save best model epoch 5, acc 0.6142
12-23 15:25:26 -----Epoch 6/99-----
12-23 15:25:26 current lr: 0.001
12-23 15:25:28 Epoch: 6 train-Loss: 0.9971 train-Acc: 0.6348, Cost 1.4960 sec
12-23 15:25:28 Epoch: 6 val-Loss: 1.0707 val-Acc: 0.6217, Cost 0.1669 sec
12-23 15:25:28 save best model epoch 6, acc 0.6217
12-23 15:25:28 -----Epoch 7/99-----
12-23 15:25:28 current lr: 0.001
12-23 15:25:30 Epoch: 7 train-Loss: 0.8509 train-Acc: 0.6985, Cost 1.6010 sec
12-23 15:25:30 Epoch: 7 val-Loss: 0.8742 val-Acc: 0.6891, Cost 0.1791 sec
12-23 15:25:30 save best model epoch 7, acc 0.6891
12-23 15:25:30 -----Epoch 8/99-----
12-23 15:25:30 current lr: 0.001
12-23 15:25:32 Epoch: 8 [896/1068], Train Loss: 0.8897 Train Acc: 0.6764,586.5 examples/sec 0.05 sec/batch
12-23 15:25:32 Epoch: 8 train-Loss: 0.8054 train-Acc: 0.6985, Cost 1.9978 sec
12-23 15:25:32 Epoch: 8 val-Loss: 0.8966 val-Acc: 0.6854, Cost 0.3034 sec
12-23 15:25:32 -----Epoch 9/99-----
12-23 15:25:32 current lr: 0.001
12-23 15:25:37 Epoch: 9 train-Loss: 0.6742 train-Acc: 0.7397, Cost 4.6060 sec
12-23 15:25:37 Epoch: 9 val-Loss: 0.8864 val-Acc: 0.7004, Cost 0.6733 sec
12-23 15:25:37 save best model epoch 9, acc 0.7004
12-23 15:25:37 -----Epoch 10/99-----
12-23 15:25:37 current lr: 0.001
12-23 15:25:43 Epoch: 10 train-Loss: 0.5792 train-Acc: 0.7931, Cost 5.8676 sec
12-23 15:25:44 Epoch: 10 val-Loss: 0.8790 val-Acc: 0.6891, Cost 0.6466 sec
12-23 15:25:44 -----Epoch 11/99-----
12-23 15:25:44 current lr: 0.001
12-23 15:25:49 Epoch: 11 [832/1068], Train Loss: 0.6143 Train Acc: 0.7694,182.9 examples/sec 0.17 sec/batch
12-23 15:25:50 Epoch: 11 train-Loss: 0.5394 train-Acc: 0.7987, Cost 5.8797 sec
12-23 15:25:51 Epoch: 11 val-Loss: 0.7963 val-Acc: 0.7341, Cost 0.6610 sec
12-23 15:25:51 save best model epoch 11, acc 0.7341
12-23 15:25:51 -----Epoch 12/99-----
12-23 15:25:51 current lr: 0.001
12-23 15:25:56 Epoch: 12 train-Loss: 0.5093 train-Acc: 0.8109, Cost 5.9557 sec
12-23 15:25:57 Epoch: 12 val-Loss: 0.9650 val-Acc: 0.6629, Cost 0.6831 sec
12-23 15:25:57 -----Epoch 13/99-----
12-23 15:25:57 current lr: 0.001
12-23 15:26:03 Epoch: 13 train-Loss: 0.4267 train-Acc: 0.8427, Cost 5.9066 sec
12-23 15:26:04 Epoch: 13 val-Loss: 0.8342 val-Acc: 0.7191, Cost 0.6630 sec
12-23 15:26:04 -----Epoch 14/99-----
12-23 15:26:04 current lr: 0.001
12-23 15:26:08 Epoch: 14 [768/1068], Train Loss: 0.4522 Train Acc: 0.8306,161.2 examples/sec 0.19 sec/batch
12-23 15:26:10 Epoch: 14 train-Loss: 0.4026 train-Acc: 0.8474, Cost 5.9032 sec
12-23 15:26:10 Epoch: 14 val-Loss: 0.7521 val-Acc: 0.7528, Cost 0.6746 sec
12-23 15:26:10 save best model epoch 14, acc 0.7528
12-23 15:26:10 -----Epoch 15/99-----
12-23 15:26:10 current lr: 0.001
12-23 15:26:16 Epoch: 15 train-Loss: 0.3148 train-Acc: 0.8933, Cost 5.9828 sec
12-23 15:26:17 Epoch: 15 val-Loss: 0.6081 val-Acc: 0.7940, Cost 0.6724 sec
12-23 15:26:17 save best model epoch 15, acc 0.7940
12-23 15:26:17 -----Epoch 16/99-----
12-23 15:26:17 current lr: 0.001
12-23 15:26:23 Epoch: 16 train-Loss: 0.2831 train-Acc: 0.8970, Cost 5.8704 sec
12-23 15:26:24 Epoch: 16 val-Loss: 0.6436 val-Acc: 0.8127, Cost 0.6720 sec
12-23 15:26:24 save best model epoch 16, acc 0.8127
12-23 15:26:24 -----Epoch 17/99-----
12-23 15:26:24 current lr: 0.001
12-23 15:26:28 Epoch: 17 [704/1068], Train Loss: 0.2975 Train Acc: 0.8962,161.5 examples/sec 0.19 sec/batch
12-23 15:26:29 Epoch: 17 train-Loss: 0.2646 train-Acc: 0.9073, Cost 5.9339 sec
12-23 15:26:30 Epoch: 17 val-Loss: 0.5687 val-Acc: 0.8427, Cost 0.6829 sec
12-23 15:26:30 save best model epoch 17, acc 0.8427
12-23 15:26:30 -----Epoch 18/99-----
12-23 15:26:30 current lr: 0.001
12-23 15:26:36 Epoch: 18 train-Loss: 0.2363 train-Acc: 0.9176, Cost 5.9936 sec
12-23 15:26:37 Epoch: 18 val-Loss: 0.6683 val-Acc: 0.7828, Cost 0.6852 sec
12-23 15:26:37 -----Epoch 19/99-----
12-23 15:26:37 current lr: 0.001
12-23 15:26:43 Epoch: 19 train-Loss: 0.2242 train-Acc: 0.9213, Cost 5.8522 sec
12-23 15:26:43 Epoch: 19 val-Loss: 0.7233 val-Acc: 0.8052, Cost 0.7058 sec
12-23 15:26:43 -----Epoch 20/99-----
12-23 15:26:43 current lr: 0.001
12-23 15:26:47 Epoch: 20 [640/1068], Train Loss: 0.2266 Train Acc: 0.9220,160.8 examples/sec 0.20 sec/batch
12-23 15:26:49 Epoch: 20 train-Loss: 0.2095 train-Acc: 0.9316, Cost 5.9240 sec
12-23 15:26:50 Epoch: 20 val-Loss: 0.7525 val-Acc: 0.7715, Cost 0.6952 sec
12-23 15:26:50 -----Epoch 21/99-----
12-23 15:26:50 current lr: 0.001
12-23 15:26:56 Epoch: 21 train-Loss: 0.1809 train-Acc: 0.9382, Cost 5.9401 sec
12-23 15:26:57 Epoch: 21 val-Loss: 0.6732 val-Acc: 0.8052, Cost 0.6925 sec
12-23 15:26:57 -----Epoch 22/99-----
12-23 15:26:57 current lr: 0.001
12-23 15:27:03 Epoch: 22 train-Loss: 0.1563 train-Acc: 0.9513, Cost 6.0028 sec
12-23 15:27:03 Epoch: 22 val-Loss: 0.7248 val-Acc: 0.7828, Cost 0.6815 sec
12-23 15:27:03 -----Epoch 23/99-----
12-23 15:27:03 current lr: 0.001
12-23 15:27:07 Epoch: 23 [576/1068], Train Loss: 0.1747 Train Acc: 0.9420,160.3 examples/sec 0.20 sec/batch
12-23 15:27:09 Epoch: 23 train-Loss: 0.1526 train-Acc: 0.9485, Cost 5.9346 sec
12-23 15:27:10 Epoch: 23 val-Loss: 0.9896 val-Acc: 0.7640, Cost 0.6826 sec
12-23 15:27:10 -----Epoch 24/99-----
12-23 15:27:10 current lr: 0.001
12-23 15:27:16 Epoch: 24 train-Loss: 0.1763 train-Acc: 0.9438, Cost 6.0695 sec
12-23 15:27:17 Epoch: 24 val-Loss: 0.6656 val-Acc: 0.8052, Cost 0.6717 sec
12-23 15:27:17 -----Epoch 25/99-----
12-23 15:27:17 current lr: 0.001
12-23 15:27:23 Epoch: 25 train-Loss: 0.1964 train-Acc: 0.9457, Cost 5.9529 sec
12-23 15:27:23 Epoch: 25 val-Loss: 0.7185 val-Acc: 0.7978, Cost 0.6737 sec
12-23 15:27:23 -----Epoch 26/99-----
12-23 15:27:23 current lr: 0.001
12-23 15:27:26 Epoch: 26 [512/1068], Train Loss: 0.1690 Train Acc: 0.9497,160.3 examples/sec 0.20 sec/batch
12-23 15:27:29 Epoch: 26 train-Loss: 0.1107 train-Acc: 0.9672, Cost 5.9303 sec
12-23 15:27:30 Epoch: 26 val-Loss: 0.7210 val-Acc: 0.8165, Cost 0.7117 sec
12-23 15:27:30 -----Epoch 27/99-----
12-23 15:27:30 current lr: 0.001
12-23 15:27:36 Epoch: 27 train-Loss: 0.0781 train-Acc: 0.9813, Cost 6.0416 sec
12-23 15:27:37 Epoch: 27 val-Loss: 0.8212 val-Acc: 0.8090, Cost 0.6921 sec
12-23 15:27:37 -----Epoch 28/99-----
12-23 15:27:37 current lr: 0.001
12-23 15:27:43 Epoch: 28 train-Loss: 0.1000 train-Acc: 0.9738, Cost 6.1204 sec
12-23 15:27:43 Epoch: 28 val-Loss: 0.9149 val-Acc: 0.7678, Cost 0.6814 sec
12-23 15:27:43 -----Epoch 29/99-----
12-23 15:27:43 current lr: 0.001
12-23 15:27:46 Epoch: 29 [448/1068], Train Loss: 0.0986 Train Acc: 0.9732,158.1 examples/sec 0.20 sec/batch
12-23 15:27:50 Epoch: 29 train-Loss: 0.1121 train-Acc: 0.9719, Cost 6.0001 sec
12-23 15:27:50 Epoch: 29 val-Loss: 0.7002 val-Acc: 0.7903, Cost 0.7147 sec
12-23 15:27:50 -----Epoch 30/99-----
12-23 15:27:50 current lr: 0.001
12-23 15:27:56 Epoch: 30 train-Loss: 0.0715 train-Acc: 0.9785, Cost 6.2121 sec
12-23 15:27:57 Epoch: 30 val-Loss: 0.7461 val-Acc: 0.8052, Cost 0.6976 sec
12-23 15:27:57 -----Epoch 31/99-----
12-23 15:27:57 current lr: 0.001
12-23 15:28:03 Epoch: 31 train-Loss: 0.0838 train-Acc: 0.9775, Cost 6.2238 sec
12-23 15:28:04 Epoch: 31 val-Loss: 0.7408 val-Acc: 0.7753, Cost 0.7335 sec
12-23 15:28:04 -----Epoch 32/99-----
12-23 15:28:04 current lr: 0.001
12-23 15:28:06 Epoch: 32 [384/1068], Train Loss: 0.0792 Train Acc: 0.9780,154.5 examples/sec 0.20 sec/batch
12-23 15:28:10 Epoch: 32 train-Loss: 0.0681 train-Acc: 0.9813, Cost 6.1496 sec
12-23 15:28:11 Epoch: 32 val-Loss: 0.8218 val-Acc: 0.8165, Cost 0.6934 sec
12-23 15:28:11 -----Epoch 33/99-----
12-23 15:28:11 current lr: 0.001
12-23 15:28:17 Epoch: 33 train-Loss: 0.0580 train-Acc: 0.9813, Cost 6.2910 sec
12-23 15:28:18 Epoch: 33 val-Loss: 0.7352 val-Acc: 0.8165, Cost 0.7270 sec
12-23 15:28:18 -----Epoch 34/99-----
12-23 15:28:18 current lr: 0.001
12-23 15:28:24 Epoch: 34 train-Loss: 0.0380 train-Acc: 0.9897, Cost 6.4132 sec
12-23 15:28:25 Epoch: 34 val-Loss: 0.7378 val-Acc: 0.8315, Cost 0.7026 sec
12-23 15:28:25 -----Epoch 35/99-----
12-23 15:28:25 current lr: 0.001
12-23 15:28:27 Epoch: 35 [320/1068], Train Loss: 0.0515 Train Acc: 0.9850,151.1 examples/sec 0.21 sec/batch
12-23 15:28:32 Epoch: 35 train-Loss: 0.0441 train-Acc: 0.9888, Cost 6.5581 sec
12-23 15:28:32 Epoch: 35 val-Loss: 0.8430 val-Acc: 0.8165, Cost 0.7456 sec
12-23 15:28:32 -----Epoch 36/99-----
12-23 15:28:32 current lr: 0.001
12-23 15:28:39 Epoch: 36 train-Loss: 0.0696 train-Acc: 0.9738, Cost 6.7614 sec
12-23 15:28:40 Epoch: 36 val-Loss: 0.8627 val-Acc: 0.8015, Cost 0.7371 sec
12-23 15:28:40 -----Epoch 37/99-----
12-23 15:28:40 current lr: 0.001
12-23 15:28:47 Epoch: 37 train-Loss: 0.0519 train-Acc: 0.9869, Cost 6.7283 sec
12-23 15:28:47 Epoch: 37 val-Loss: 0.8196 val-Acc: 0.8202, Cost 0.7425 sec
12-23 15:28:47 -----Epoch 38/99-----
12-23 15:28:47 current lr: 0.001
12-23 15:28:49 Epoch: 38 [256/1068], Train Loss: 0.0610 Train Acc: 0.9812,143.5 examples/sec 0.22 sec/batch
12-23 15:28:54 Epoch: 38 train-Loss: 0.0681 train-Acc: 0.9813, Cost 6.6155 sec
12-23 15:28:55 Epoch: 38 val-Loss: 0.7666 val-Acc: 0.8052, Cost 0.7229 sec
12-23 15:28:55 -----Epoch 39/99-----
12-23 15:28:55 current lr: 0.001
12-23 15:29:02 Epoch: 39 train-Loss: 0.0449 train-Acc: 0.9897, Cost 6.8790 sec
12-23 15:29:02 Epoch: 39 val-Loss: 0.7400 val-Acc: 0.8315, Cost 0.7271 sec
12-23 15:29:02 -----Epoch 40/99-----
12-23 15:29:02 current lr: 0.001
12-23 15:29:09 Epoch: 40 train-Loss: 0.0381 train-Acc: 0.9888, Cost 6.9860 sec
12-23 15:29:10 Epoch: 40 val-Loss: 0.8116 val-Acc: 0.7903, Cost 0.7816 sec
12-23 15:29:10 -----Epoch 41/99-----
12-23 15:29:10 current lr: 0.001
12-23 15:29:12 Epoch: 41 [192/1068], Train Loss: 0.0491 Train Acc: 0.9876,140.0 examples/sec 0.22 sec/batch
12-23 15:29:17 Epoch: 41 train-Loss: 0.0613 train-Acc: 0.9813, Cost 7.1400 sec
12-23 15:29:18 Epoch: 41 val-Loss: 0.8095 val-Acc: 0.8277, Cost 0.8118 sec
12-23 15:29:18 -----Epoch 42/99-----
12-23 15:29:18 current lr: 0.001
12-23 15:29:25 Epoch: 42 train-Loss: 0.0729 train-Acc: 0.9785, Cost 7.2013 sec
12-23 15:29:26 Epoch: 42 val-Loss: 0.8034 val-Acc: 0.8127, Cost 0.8692 sec
12-23 15:29:26 -----Epoch 43/99-----
12-23 15:29:26 current lr: 0.001
12-23 15:29:34 Epoch: 43 train-Loss: 0.0551 train-Acc: 0.9813, Cost 7.4961 sec
12-23 15:29:35 Epoch: 43 val-Loss: 0.8557 val-Acc: 0.8165, Cost 0.9325 sec
12-23 15:29:35 -----Epoch 44/99-----
12-23 15:29:35 current lr: 0.001
12-23 15:29:36 Epoch: 44 [128/1068], Train Loss: 0.0614 Train Acc: 0.9806,130.1 examples/sec 0.24 sec/batch
12-23 15:29:42 Epoch: 44 train-Loss: 0.0382 train-Acc: 0.9916, Cost 7.5634 sec
12-23 15:29:43 Epoch: 44 val-Loss: 1.0164 val-Acc: 0.7865, Cost 0.9905 sec
12-23 15:29:43 -----Epoch 45/99-----
12-23 15:29:43 current lr: 0.001
12-23 15:29:51 Epoch: 45 train-Loss: 0.0598 train-Acc: 0.9831, Cost 7.8819 sec
12-23 15:29:52 Epoch: 45 val-Loss: 0.7947 val-Acc: 0.8352, Cost 1.0551 sec
12-23 15:29:52 -----Epoch 46/99-----
12-23 15:29:52 current lr: 0.001
12-23 15:30:00 Epoch: 46 train-Loss: 0.0461 train-Acc: 0.9841, Cost 7.5693 sec
12-23 15:30:01 Epoch: 46 val-Loss: 0.8942 val-Acc: 0.8090, Cost 1.0743 sec
12-23 15:30:01 -----Epoch 47/99-----
12-23 15:30:01 current lr: 0.001
12-23 15:30:01 Epoch: 47 [64/1068], Train Loss: 0.0473 Train Acc: 0.9866,122.2 examples/sec 0.26 sec/batch
12-23 15:30:09 Epoch: 47 train-Loss: 0.0765 train-Acc: 0.9813, Cost 7.9043 sec
12-23 15:30:10 Epoch: 47 val-Loss: 0.8963 val-Acc: 0.7865, Cost 1.0349 sec
12-23 15:30:10 -----Epoch 48/99-----
12-23 15:30:10 current lr: 0.001
12-23 15:30:17 Epoch: 48 train-Loss: 0.0617 train-Acc: 0.9822, Cost 7.4873 sec
12-23 15:30:18 Epoch: 48 val-Loss: 0.9065 val-Acc: 0.8202, Cost 1.0282 sec
12-23 15:30:18 -----Epoch 49/99-----
12-23 15:30:18 current lr: 0.001
12-23 15:30:26 Epoch: 49 train-Loss: 0.0564 train-Acc: 0.9841, Cost 8.0211 sec
12-23 15:30:27 Epoch: 49 val-Loss: 0.8652 val-Acc: 0.7903, Cost 1.0529 sec
12-23 15:30:27 -----Epoch 50/99-----
12-23 15:30:27 current lr: 0.001
12-23 15:30:28 Epoch: 50 [0/1068], Train Loss: 0.0672 Train Acc: 0.9815,120.3 examples/sec 0.26 sec/batch
12-23 15:30:35 Epoch: 50 train-Loss: 0.0584 train-Acc: 0.9785, Cost 7.9840 sec
12-23 15:30:36 Epoch: 50 val-Loss: 0.7903 val-Acc: 0.8090, Cost 1.0213 sec
12-23 15:30:36 -----Epoch 51/99-----
12-23 15:30:36 current lr: 0.001
12-23 15:30:44 Epoch: 51 train-Loss: 0.0655 train-Acc: 0.9775, Cost 7.8080 sec
12-23 15:30:44 Epoch: 51 val-Loss: 0.7963 val-Acc: 0.8277, Cost 0.4183 sec
12-23 15:30:44 -----Epoch 52/99-----
12-23 15:30:44 current lr: 0.001
12-23 15:30:47 Epoch: 52 [1024/1068], Train Loss: 0.0576 Train Acc: 0.9791,160.0 examples/sec 0.20 sec/batch
12-23 15:30:47 Epoch: 52 train-Loss: 0.0510 train-Acc: 0.9803, Cost 2.8282 sec
12-23 15:30:48 Epoch: 52 val-Loss: 0.8703 val-Acc: 0.8090, Cost 0.3728 sec
12-23 15:30:48 -----Epoch 53/99-----
12-23 15:30:48 current lr: 0.001
12-23 15:30:53 Epoch: 53 train-Loss: 0.0209 train-Acc: 0.9953, Cost 5.3918 sec
12-23 15:30:54 Epoch: 53 val-Loss: 0.8069 val-Acc: 0.8352, Cost 1.0621 sec
12-23 15:30:54 -----Epoch 54/99-----
12-23 15:30:54 current lr: 0.001
12-23 15:31:02 Epoch: 54 train-Loss: 0.0334 train-Acc: 0.9869, Cost 7.9699 sec
12-23 15:31:03 Epoch: 54 val-Loss: 0.9588 val-Acc: 0.7753, Cost 0.9717 sec
12-23 15:31:03 -----Epoch 55/99-----
12-23 15:31:03 current lr: 0.001
12-23 15:31:11 Epoch: 55 [960/1068], Train Loss: 0.0267 Train Acc: 0.9911,134.6 examples/sec 0.23 sec/batch
12-23 15:31:11 Epoch: 55 train-Loss: 0.0272 train-Acc: 0.9906, Cost 8.1238 sec
12-23 15:31:12 Epoch: 55 val-Loss: 0.8638 val-Acc: 0.8090, Cost 1.0628 sec
12-23 15:31:12 -----Epoch 56/99-----
12-23 15:31:12 current lr: 0.001
12-23 15:31:21 Epoch: 56 train-Loss: 0.0330 train-Acc: 0.9906, Cost 8.2599 sec
12-23 15:31:22 Epoch: 56 val-Loss: 0.8000 val-Acc: 0.8090, Cost 1.1444 sec
12-23 15:31:22 -----Epoch 57/99-----
12-23 15:31:22 current lr: 0.001
12-23 15:31:26 Epoch: 57 train-Loss: 0.0371 train-Acc: 0.9878, Cost 4.6216 sec
12-23 15:31:27 Epoch: 57 val-Loss: 0.9925 val-Acc: 0.7940, Cost 0.3373 sec
12-23 15:31:27 -----Epoch 58/99-----
12-23 15:31:27 current lr: 0.001
12-23 15:31:29 Epoch: 58 [896/1068], Train Loss: 0.0343 Train Acc: 0.9892,171.1 examples/sec 0.18 sec/batch
12-23 15:31:29 Epoch: 58 train-Loss: 0.0297 train-Acc: 0.9906, Cost 2.6627 sec
12-23 15:31:30 Epoch: 58 val-Loss: 0.8843 val-Acc: 0.8052, Cost 0.3405 sec
12-23 15:31:30 -----Epoch 59/99-----
12-23 15:31:30 current lr: 0.001
12-23 15:31:33 Epoch: 59 train-Loss: 0.0308 train-Acc: 0.9888, Cost 3.6980 sec
12-23 15:31:34 Epoch: 59 val-Loss: 1.0319 val-Acc: 0.8015, Cost 0.6010 sec
12-23 15:31:34 -----Epoch 60/99-----
12-23 15:31:34 current lr: 0.001
12-23 15:31:38 Epoch: 60 train-Loss: 0.0289 train-Acc: 0.9888, Cost 4.0520 sec
12-23 15:31:39 Epoch: 60 val-Loss: 0.9180 val-Acc: 0.8165, Cost 0.5964 sec
12-23 15:31:39 -----Epoch 61/99-----
12-23 15:31:39 current lr: 0.001
12-23 15:31:42 Epoch: 61 [832/1068], Train Loss: 0.0275 Train Acc: 0.9895,236.1 examples/sec 0.13 sec/batch
12-23 15:31:43 Epoch: 61 train-Loss: 0.0232 train-Acc: 0.9906, Cost 4.4381 sec
12-23 15:31:44 Epoch: 61 val-Loss: 0.8530 val-Acc: 0.8240, Cost 0.6087 sec
12-23 15:31:44 -----Epoch 62/99-----
12-23 15:31:44 current lr: 0.001
12-23 15:31:48 Epoch: 62 train-Loss: 0.0264 train-Acc: 0.9916, Cost 4.1122 sec
12-23 15:31:48 Epoch: 62 val-Loss: 0.9171 val-Acc: 0.7940, Cost 0.5674 sec
12-23 15:31:48 -----Epoch 63/99-----
12-23 15:31:48 current lr: 0.001
12-23 15:31:52 Epoch: 63 train-Loss: 0.0519 train-Acc: 0.9878, Cost 4.1783 sec
12-23 15:31:53 Epoch: 63 val-Loss: 0.9436 val-Acc: 0.7978, Cost 0.5555 sec
12-23 15:31:53 -----Epoch 64/99-----
12-23 15:31:53 current lr: 0.001
12-23 15:31:56 Epoch: 64 [768/1068], Train Loss: 0.0383 Train Acc: 0.9901,229.0 examples/sec 0.14 sec/batch
12-23 15:31:57 Epoch: 64 train-Loss: 0.0482 train-Acc: 0.9869, Cost 3.9287 sec
12-23 15:31:57 Epoch: 64 val-Loss: 0.9633 val-Acc: 0.7640, Cost 0.5140 sec
12-23 15:31:57 -----Epoch 65/99-----
12-23 15:31:57 current lr: 0.001
12-23 15:32:02 Epoch: 65 train-Loss: 0.0535 train-Acc: 0.9813, Cost 4.2127 sec
12-23 15:32:02 Epoch: 65 val-Loss: 0.9269 val-Acc: 0.8315, Cost 0.5435 sec
12-23 15:32:02 -----Epoch 66/99-----
12-23 15:32:02 current lr: 0.001
12-23 15:32:06 Epoch: 66 train-Loss: 0.0494 train-Acc: 0.9850, Cost 3.9734 sec
12-23 15:32:07 Epoch: 66 val-Loss: 0.9236 val-Acc: 0.8277, Cost 0.5775 sec
12-23 15:32:07 -----Epoch 67/99-----
12-23 15:32:07 current lr: 0.001
12-23 15:32:10 Epoch: 67 [704/1068], Train Loss: 0.0559 Train Acc: 0.9834,231.0 examples/sec 0.14 sec/batch
12-23 15:32:11 Epoch: 67 train-Loss: 0.0647 train-Acc: 0.9850, Cost 3.9781 sec
12-23 15:32:11 Epoch: 67 val-Loss: 0.8987 val-Acc: 0.8127, Cost 0.5467 sec
12-23 15:32:11 -----Epoch 68/99-----
12-23 15:32:11 current lr: 0.001
12-23 15:32:15 Epoch: 68 train-Loss: 0.0408 train-Acc: 0.9888, Cost 4.0168 sec
12-23 15:32:16 Epoch: 68 val-Loss: 0.8489 val-Acc: 0.8240, Cost 0.5348 sec
12-23 15:32:16 -----Epoch 69/99-----
12-23 15:32:16 current lr: 0.001
12-23 15:32:20 Epoch: 69 train-Loss: 0.0289 train-Acc: 0.9906, Cost 3.9840 sec
12-23 15:32:20 Epoch: 69 val-Loss: 0.8239 val-Acc: 0.8502, Cost 0.5213 sec
12-23 15:32:20 save best model epoch 69, acc 0.8502
12-23 15:32:20 -----Epoch 70/99-----
12-23 15:32:20 current lr: 0.001
12-23 15:32:23 Epoch: 70 [640/1068], Train Loss: 0.0411 Train Acc: 0.9885,236.2 examples/sec 0.13 sec/batch
12-23 15:32:24 Epoch: 70 train-Loss: 0.0508 train-Acc: 0.9878, Cost 3.9989 sec
12-23 15:32:25 Epoch: 70 val-Loss: 0.9388 val-Acc: 0.7978, Cost 0.5483 sec
12-23 15:32:25 -----Epoch 71/99-----
12-23 15:32:25 current lr: 0.001
12-23 15:32:29 Epoch: 71 train-Loss: 0.0321 train-Acc: 0.9897, Cost 3.9865 sec
12-23 15:32:29 Epoch: 71 val-Loss: 0.9500 val-Acc: 0.8015, Cost 0.5464 sec
12-23 15:32:29 -----Epoch 72/99-----
12-23 15:32:29 current lr: 0.001
12-23 15:32:34 Epoch: 72 train-Loss: 0.0303 train-Acc: 0.9897, Cost 4.0953 sec
12-23 15:32:34 Epoch: 72 val-Loss: 1.2104 val-Acc: 0.8090, Cost 0.5470 sec
12-23 15:32:34 -----Epoch 73/99-----
12-23 15:32:34 current lr: 0.001
12-23 15:32:36 Epoch: 73 [576/1068], Train Loss: 0.0402 Train Acc: 0.9869,232.0 examples/sec 0.14 sec/batch
12-23 15:32:38 Epoch: 73 train-Loss: 0.0561 train-Acc: 0.9813, Cost 3.9123 sec
12-23 15:32:39 Epoch: 73 val-Loss: 0.9657 val-Acc: 0.8052, Cost 0.5615 sec
12-23 15:32:39 -----Epoch 74/99-----
12-23 15:32:39 current lr: 0.001
12-23 15:32:43 Epoch: 74 train-Loss: 0.0566 train-Acc: 0.9794, Cost 4.0602 sec
12-23 15:32:43 Epoch: 74 val-Loss: 0.9074 val-Acc: 0.8315, Cost 0.5711 sec
12-23 15:32:43 -----Epoch 75/99-----
12-23 15:32:43 current lr: 0.001
12-23 15:32:47 Epoch: 75 train-Loss: 0.0881 train-Acc: 0.9728, Cost 4.2509 sec
12-23 15:32:48 Epoch: 75 val-Loss: 0.9335 val-Acc: 0.8165, Cost 0.5728 sec
12-23 15:32:48 -----Epoch 76/99-----
12-23 15:32:48 current lr: 0.001
12-23 15:32:50 Epoch: 76 [512/1068], Train Loss: 0.0763 Train Acc: 0.9755,227.4 examples/sec 0.14 sec/batch
12-23 15:32:52 Epoch: 76 train-Loss: 0.1210 train-Acc: 0.9588, Cost 4.3482 sec
12-23 15:32:53 Epoch: 76 val-Loss: 1.0952 val-Acc: 0.7678, Cost 0.6362 sec
12-23 15:32:53 -----Epoch 77/99-----
12-23 15:32:53 current lr: 0.001
12-23 15:32:57 Epoch: 77 train-Loss: 0.1407 train-Acc: 0.9504, Cost 4.4317 sec
12-23 15:32:58 Epoch: 77 val-Loss: 1.2897 val-Acc: 0.7116, Cost 0.6655 sec
12-23 15:32:58 -----Epoch 78/99-----
12-23 15:32:58 current lr: 0.001
12-23 15:33:02 Epoch: 78 train-Loss: 0.1207 train-Acc: 0.9635, Cost 4.0689 sec
12-23 15:33:03 Epoch: 78 val-Loss: 1.1226 val-Acc: 0.7715, Cost 0.5594 sec
12-23 15:33:03 -----Epoch 79/99-----
12-23 15:33:03 current lr: 0.001
12-23 15:33:05 Epoch: 79 [448/1068], Train Loss: 0.1213 Train Acc: 0.9599,218.4 examples/sec 0.14 sec/batch
12-23 15:33:07 Epoch: 79 train-Loss: 0.0660 train-Acc: 0.9803, Cost 3.8910 sec
12-23 15:33:07 Epoch: 79 val-Loss: 1.0843 val-Acc: 0.7903, Cost 0.5682 sec
12-23 15:33:07 -----Epoch 80/99-----
12-23 15:33:07 current lr: 0.001
12-23 15:33:11 Epoch: 80 train-Loss: 0.0447 train-Acc: 0.9878, Cost 4.2073 sec
12-23 15:33:12 Epoch: 80 val-Loss: 1.0124 val-Acc: 0.8052, Cost 0.5209 sec
12-23 15:33:12 -----Epoch 81/99-----
12-23 15:33:12 current lr: 0.001
12-23 15:33:16 Epoch: 81 train-Loss: 0.0288 train-Acc: 0.9860, Cost 3.9033 sec
12-23 15:33:16 Epoch: 81 val-Loss: 0.8659 val-Acc: 0.8240, Cost 0.5160 sec
12-23 15:33:16 -----Epoch 82/99-----
12-23 15:33:16 current lr: 0.001
12-23 15:33:18 Epoch: 82 [384/1068], Train Loss: 0.0392 Train Acc: 0.9869,232.5 examples/sec 0.14 sec/batch
12-23 15:33:21 Epoch: 82 train-Loss: 0.0256 train-Acc: 0.9906, Cost 4.2276 sec
12-23 15:33:21 Epoch: 82 val-Loss: 1.0018 val-Acc: 0.8277, Cost 0.5696 sec
12-23 15:33:21 -----Epoch 83/99-----
12-23 15:33:21 current lr: 0.001
12-23 15:33:26 Epoch: 83 train-Loss: 0.0431 train-Acc: 0.9831, Cost 4.8846 sec
12-23 15:33:27 Epoch: 83 val-Loss: 0.9955 val-Acc: 0.8240, Cost 0.5179 sec
12-23 15:33:27 -----Epoch 84/99-----
12-23 15:33:27 current lr: 0.001
12-23 15:33:31 Epoch: 84 train-Loss: 0.0649 train-Acc: 0.9785, Cost 4.5085 sec
12-23 15:33:32 Epoch: 84 val-Loss: 1.0770 val-Acc: 0.8052, Cost 0.5783 sec
12-23 15:33:32 -----Epoch 85/99-----
12-23 15:33:32 current lr: 0.001
12-23 15:33:33 Epoch: 85 [320/1068], Train Loss: 0.0488 Train Acc: 0.9825,209.3 examples/sec 0.15 sec/batch
12-23 15:33:36 Epoch: 85 train-Loss: 0.0441 train-Acc: 0.9860, Cost 4.6457 sec
12-23 15:33:37 Epoch: 85 val-Loss: 0.9666 val-Acc: 0.8315, Cost 0.5868 sec
12-23 15:33:37 -----Epoch 86/99-----
12-23 15:33:37 current lr: 0.001
12-23 15:33:41 Epoch: 86 train-Loss: 0.0221 train-Acc: 0.9916, Cost 4.4018 sec
12-23 15:33:42 Epoch: 86 val-Loss: 0.8775 val-Acc: 0.8202, Cost 0.7050 sec
12-23 15:33:42 -----Epoch 87/99-----
12-23 15:33:42 current lr: 0.001
12-23 15:33:47 Epoch: 87 train-Loss: 0.0356 train-Acc: 0.9916, Cost 4.9896 sec
12-23 15:33:48 Epoch: 87 val-Loss: 0.9074 val-Acc: 0.8315, Cost 0.7391 sec
12-23 15:33:48 -----Epoch 88/99-----
12-23 15:33:48 current lr: 0.001
12-23 15:33:49 Epoch: 88 [256/1068], Train Loss: 0.0296 Train Acc: 0.9911,197.2 examples/sec 0.16 sec/batch
12-23 15:33:52 Epoch: 88 train-Loss: 0.0166 train-Acc: 0.9944, Cost 4.4050 sec
12-23 15:33:53 Epoch: 88 val-Loss: 0.9590 val-Acc: 0.8277, Cost 0.5663 sec
12-23 15:33:53 -----Epoch 89/99-----
12-23 15:33:53 current lr: 0.001
12-23 15:33:57 Epoch: 89 train-Loss: 0.0248 train-Acc: 0.9934, Cost 4.2201 sec
12-23 15:33:58 Epoch: 89 val-Loss: 0.9141 val-Acc: 0.8240, Cost 0.5469 sec
12-23 15:33:58 -----Epoch 90/99-----
12-23 15:33:58 current lr: 0.001
12-23 15:34:02 Epoch: 90 train-Loss: 0.0282 train-Acc: 0.9934, Cost 4.1276 sec
12-23 15:34:02 Epoch: 90 val-Loss: 0.9513 val-Acc: 0.8352, Cost 0.5699 sec
12-23 15:34:02 -----Epoch 91/99-----
12-23 15:34:02 current lr: 0.001
12-23 15:34:03 Epoch: 91 [192/1068], Train Loss: 0.0237 Train Acc: 0.9933,223.0 examples/sec 0.14 sec/batch
12-23 15:34:06 Epoch: 91 train-Loss: 0.0483 train-Acc: 0.9869, Cost 4.1479 sec
12-23 15:34:07 Epoch: 91 val-Loss: 0.9630 val-Acc: 0.8165, Cost 0.5832 sec
12-23 15:34:07 -----Epoch 92/99-----
12-23 15:34:07 current lr: 0.001
12-23 15:34:11 Epoch: 92 train-Loss: 0.0393 train-Acc: 0.9897, Cost 4.2493 sec
12-23 15:34:12 Epoch: 92 val-Loss: 1.0240 val-Acc: 0.8202, Cost 0.5988 sec
12-23 15:34:12 -----Epoch 93/99-----
12-23 15:34:12 current lr: 0.001
12-23 15:34:16 Epoch: 93 train-Loss: 0.0231 train-Acc: 0.9925, Cost 4.1849 sec
12-23 15:34:17 Epoch: 93 val-Loss: 1.1030 val-Acc: 0.8240, Cost 0.6438 sec
12-23 15:34:17 -----Epoch 94/99-----
12-23 15:34:17 current lr: 0.001
12-23 15:34:17 Epoch: 94 [128/1068], Train Loss: 0.0376 Train Acc: 0.9895,220.0 examples/sec 0.14 sec/batch
12-23 15:34:21 Epoch: 94 train-Loss: 0.0186 train-Acc: 0.9916, Cost 4.6871 sec
12-23 15:34:22 Epoch: 94 val-Loss: 1.0047 val-Acc: 0.8052, Cost 0.7535 sec
12-23 15:34:22 -----Epoch 95/99-----
12-23 15:34:22 current lr: 0.001
12-23 15:34:26 Epoch: 95 train-Loss: 0.0230 train-Acc: 0.9906, Cost 4.4045 sec
12-23 15:34:27 Epoch: 95 val-Loss: 0.9513 val-Acc: 0.8352, Cost 0.5553 sec
12-23 15:34:27 -----Epoch 96/99-----
12-23 15:34:27 current lr: 0.001
12-23 15:34:31 Epoch: 96 train-Loss: 0.0252 train-Acc: 0.9916, Cost 4.1774 sec
12-23 15:34:32 Epoch: 96 val-Loss: 0.9762 val-Acc: 0.8202, Cost 0.7261 sec
12-23 15:34:32 -----Epoch 97/99-----
12-23 15:34:32 current lr: 0.001
12-23 15:34:32 Epoch: 97 [64/1068], Train Loss: 0.0218 Train Acc: 0.9917,208.5 examples/sec 0.15 sec/batch
12-23 15:34:37 Epoch: 97 train-Loss: 0.0194 train-Acc: 0.9925, Cost 4.6175 sec
12-23 15:34:37 Epoch: 97 val-Loss: 0.9517 val-Acc: 0.8315, Cost 0.7079 sec
12-23 15:34:37 -----Epoch 98/99-----
12-23 15:34:37 current lr: 0.001
12-23 15:34:41 Epoch: 98 train-Loss: 0.0343 train-Acc: 0.9897, Cost 4.2164 sec
12-23 15:34:42 Epoch: 98 val-Loss: 0.8775 val-Acc: 0.8315, Cost 0.6106 sec
12-23 15:34:42 -----Epoch 99/99-----
12-23 15:34:42 current lr: 0.001
12-23 15:34:46 Epoch: 99 train-Loss: 0.0237 train-Acc: 0.9925, Cost 4.2000 sec
12-23 15:34:47 Epoch: 99 val-Loss: 0.8546 val-Acc: 0.8539, Cost 0.6173 sec
12-23 15:34:47 save best model epoch 99, acc 0.8539
