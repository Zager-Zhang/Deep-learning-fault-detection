12-23 15:20:01 model_name: cnn_1d
12-23 15:20:01 data_name: CWRU
12-23 15:20:01 data_dir: C:\Users\ZAGER\Desktop\DL-based-Intelligent-Diagnosis-Benchmark-master\cwru
12-23 15:20:01 normlizetype: 0-1
12-23 15:20:01 processing_type: R_A
12-23 15:20:01 cuda_device: 0
12-23 15:20:01 checkpoint_dir: ./checkpoint
12-23 15:20:01 pretrained: True
12-23 15:20:01 batch_size: 64
12-23 15:20:01 num_workers: 0
12-23 15:20:01 opt: adam
12-23 15:20:01 lr: 0.001
12-23 15:20:01 momentum: 0.9
12-23 15:20:01 weight_decay: 1e-05
12-23 15:20:01 lr_scheduler: fix
12-23 15:20:01 gamma: 0.1
12-23 15:20:01 steps: 9
12-23 15:20:01 max_epoch: 100
12-23 15:20:01 print_step: 100
12-23 15:20:01 using 1 gpus
12-23 15:20:03 -----Epoch 0/99-----
12-23 15:20:03 current lr: 0.001
12-23 15:20:08 Epoch: 0 [0/1044], Train Loss: 2.3158 Train Acc: 0.0781,12.1 examples/sec 5.28 sec/batch
12-23 15:20:08 Epoch: 0 train-Loss: 2.0749 train-Acc: 0.2462, Cost 5.6271 sec
12-23 15:20:08 Epoch: 0 val-Loss: 2.2212 val-Acc: 0.2490, Cost 0.0380 sec
12-23 15:20:08 save best model epoch 0, acc 0.2490
12-23 15:20:08 -----Epoch 1/99-----
12-23 15:20:08 current lr: 0.001
12-23 15:20:09 Epoch: 1 train-Loss: 1.5631 train-Acc: 0.4550, Cost 0.3696 sec
12-23 15:20:09 Epoch: 1 val-Loss: 1.6297 val-Acc: 0.5096, Cost 0.0370 sec
12-23 15:20:09 save best model epoch 1, acc 0.5096
12-23 15:20:09 -----Epoch 2/99-----
12-23 15:20:09 current lr: 0.001
12-23 15:20:09 Epoch: 2 train-Loss: 1.2250 train-Acc: 0.6044, Cost 0.3676 sec
12-23 15:20:09 Epoch: 2 val-Loss: 1.0255 val-Acc: 0.7203, Cost 0.0350 sec
12-23 15:20:09 save best model epoch 2, acc 0.7203
12-23 15:20:09 -----Epoch 3/99-----
12-23 15:20:09 current lr: 0.001
12-23 15:20:10 Epoch: 3 train-Loss: 0.9218 train-Acc: 0.6715, Cost 0.3806 sec
12-23 15:20:10 Epoch: 3 val-Loss: 0.6512 val-Acc: 0.7203, Cost 0.0340 sec
12-23 15:20:10 -----Epoch 4/99-----
12-23 15:20:10 current lr: 0.001
12-23 15:20:10 Epoch: 4 train-Loss: 0.6590 train-Acc: 0.7452, Cost 0.3706 sec
12-23 15:20:10 Epoch: 4 val-Loss: 0.4910 val-Acc: 0.7778, Cost 0.0350 sec
12-23 15:20:10 save best model epoch 4, acc 0.7778
12-23 15:20:10 -----Epoch 5/99-----
12-23 15:20:10 current lr: 0.001
12-23 15:20:10 Epoch: 5 [960/1044], Train Loss: 1.1640 Train Acc: 0.5846,2573.4 examples/sec 0.02 sec/batch
12-23 15:20:10 Epoch: 5 train-Loss: 0.5997 train-Acc: 0.7557, Cost 0.3686 sec
12-23 15:20:10 Epoch: 5 val-Loss: 0.4411 val-Acc: 0.8314, Cost 0.0350 sec
12-23 15:20:10 save best model epoch 5, acc 0.8314
12-23 15:20:10 -----Epoch 6/99-----
12-23 15:20:10 current lr: 0.001
12-23 15:20:11 Epoch: 6 train-Loss: 0.5044 train-Acc: 0.8075, Cost 0.3736 sec
12-23 15:20:11 Epoch: 6 val-Loss: 0.3443 val-Acc: 0.8697, Cost 0.0350 sec
12-23 15:20:11 save best model epoch 6, acc 0.8697
12-23 15:20:11 -----Epoch 7/99-----
12-23 15:20:11 current lr: 0.001
12-23 15:20:11 Epoch: 7 train-Loss: 0.3748 train-Acc: 0.8640, Cost 0.3656 sec
12-23 15:20:11 Epoch: 7 val-Loss: 0.3470 val-Acc: 0.8736, Cost 0.0350 sec
12-23 15:20:11 save best model epoch 7, acc 0.8736
12-23 15:20:11 -----Epoch 8/99-----
12-23 15:20:11 current lr: 0.001
12-23 15:20:12 Epoch: 8 train-Loss: 0.3535 train-Acc: 0.8688, Cost 0.3676 sec
12-23 15:20:12 Epoch: 8 val-Loss: 0.2818 val-Acc: 0.9272, Cost 0.0360 sec
12-23 15:20:12 save best model epoch 8, acc 0.9272
12-23 15:20:12 -----Epoch 9/99-----
12-23 15:20:12 current lr: 0.001
12-23 15:20:12 Epoch: 9 train-Loss: 0.3833 train-Acc: 0.8563, Cost 0.3706 sec
12-23 15:20:12 Epoch: 9 val-Loss: 0.2945 val-Acc: 0.8927, Cost 0.0380 sec
12-23 15:20:12 -----Epoch 10/99-----
12-23 15:20:12 current lr: 0.001
12-23 15:20:12 Epoch: 10 train-Loss: 0.3452 train-Acc: 0.8764, Cost 0.3776 sec
12-23 15:20:12 Epoch: 10 val-Loss: 0.2057 val-Acc: 0.9464, Cost 0.0360 sec
12-23 15:20:12 save best model epoch 10, acc 0.9464
12-23 15:20:12 -----Epoch 11/99-----
12-23 15:20:12 current lr: 0.001
12-23 15:20:13 Epoch: 11 [832/1044], Train Loss: 0.3796 Train Acc: 0.8611,2530.9 examples/sec 0.02 sec/batch
12-23 15:20:13 Epoch: 11 train-Loss: 0.2918 train-Acc: 0.9042, Cost 0.3736 sec
12-23 15:20:13 Epoch: 11 val-Loss: 0.1754 val-Acc: 0.9464, Cost 0.0370 sec
12-23 15:20:13 -----Epoch 12/99-----
12-23 15:20:13 current lr: 0.001
12-23 15:20:13 Epoch: 12 train-Loss: 0.2771 train-Acc: 0.8966, Cost 0.3786 sec
12-23 15:20:13 Epoch: 12 val-Loss: 0.1932 val-Acc: 0.9157, Cost 0.0360 sec
12-23 15:20:13 -----Epoch 13/99-----
12-23 15:20:13 current lr: 0.001
12-23 15:20:14 Epoch: 13 train-Loss: 0.2396 train-Acc: 0.9138, Cost 0.3896 sec
12-23 15:20:14 Epoch: 13 val-Loss: 0.1554 val-Acc: 0.9272, Cost 0.0360 sec
12-23 15:20:14 -----Epoch 14/99-----
12-23 15:20:14 current lr: 0.001
12-23 15:20:14 Epoch: 14 train-Loss: 0.3128 train-Acc: 0.8831, Cost 0.3716 sec
12-23 15:20:14 Epoch: 14 val-Loss: 0.4343 val-Acc: 0.8123, Cost 0.0360 sec
12-23 15:20:14 -----Epoch 15/99-----
12-23 15:20:14 current lr: 0.001
12-23 15:20:14 Epoch: 15 train-Loss: 0.2246 train-Acc: 0.9119, Cost 0.3836 sec
12-23 15:20:15 Epoch: 15 val-Loss: 0.1747 val-Acc: 0.9349, Cost 0.0350 sec
12-23 15:20:15 -----Epoch 16/99-----
12-23 15:20:15 current lr: 0.001
12-23 15:20:15 Epoch: 16 train-Loss: 0.1751 train-Acc: 0.9406, Cost 0.3806 sec
12-23 15:20:15 Epoch: 16 val-Loss: 0.1138 val-Acc: 0.9617, Cost 0.0360 sec
12-23 15:20:15 save best model epoch 16, acc 0.9617
12-23 15:20:15 -----Epoch 17/99-----
12-23 15:20:15 current lr: 0.001
12-23 15:20:15 Epoch: 17 [704/1044], Train Loss: 0.2325 Train Acc: 0.9135,2495.9 examples/sec 0.02 sec/batch
12-23 15:20:15 Epoch: 17 train-Loss: 0.1552 train-Acc: 0.9406, Cost 0.3766 sec
12-23 15:20:15 Epoch: 17 val-Loss: 0.5255 val-Acc: 0.8046, Cost 0.0360 sec
12-23 15:20:15 -----Epoch 18/99-----
12-23 15:20:15 current lr: 0.001
12-23 15:20:16 Epoch: 18 train-Loss: 0.1519 train-Acc: 0.9444, Cost 0.3676 sec
12-23 15:20:16 Epoch: 18 val-Loss: 0.2852 val-Acc: 0.8889, Cost 0.0360 sec
12-23 15:20:16 -----Epoch 19/99-----
12-23 15:20:16 current lr: 0.001
12-23 15:20:16 Epoch: 19 train-Loss: 0.1697 train-Acc: 0.9377, Cost 0.3716 sec
12-23 15:20:16 Epoch: 19 val-Loss: 0.2433 val-Acc: 0.9234, Cost 0.0360 sec
12-23 15:20:16 -----Epoch 20/99-----
12-23 15:20:16 current lr: 0.001
12-23 15:20:17 Epoch: 20 train-Loss: 0.1998 train-Acc: 0.9243, Cost 0.3686 sec
12-23 15:20:17 Epoch: 20 val-Loss: 0.1216 val-Acc: 0.9425, Cost 0.0400 sec
12-23 15:20:17 -----Epoch 21/99-----
12-23 15:20:17 current lr: 0.001
12-23 15:20:17 Epoch: 21 train-Loss: 0.1597 train-Acc: 0.9349, Cost 0.3936 sec
12-23 15:20:17 Epoch: 21 val-Loss: 0.1138 val-Acc: 0.9425, Cost 0.0370 sec
12-23 15:20:17 -----Epoch 22/99-----
12-23 15:20:17 current lr: 0.001
12-23 15:20:17 Epoch: 22 train-Loss: 0.1470 train-Acc: 0.9531, Cost 0.3796 sec
12-23 15:20:17 Epoch: 22 val-Loss: 0.6793 val-Acc: 0.8008, Cost 0.0350 sec
12-23 15:20:17 -----Epoch 23/99-----
12-23 15:20:17 current lr: 0.001
12-23 15:20:18 Epoch: 23 [576/1044], Train Loss: 0.1649 Train Acc: 0.9394,2531.9 examples/sec 0.02 sec/batch
12-23 15:20:18 Epoch: 23 train-Loss: 0.1317 train-Acc: 0.9521, Cost 0.3566 sec
12-23 15:20:18 Epoch: 23 val-Loss: 0.0971 val-Acc: 0.9579, Cost 0.0370 sec
12-23 15:20:18 -----Epoch 24/99-----
12-23 15:20:18 current lr: 0.001
12-23 15:20:18 Epoch: 24 train-Loss: 0.0944 train-Acc: 0.9693, Cost 0.3666 sec
12-23 15:20:18 Epoch: 24 val-Loss: 0.1169 val-Acc: 0.9655, Cost 0.0350 sec
12-23 15:20:18 save best model epoch 24, acc 0.9655
12-23 15:20:18 -----Epoch 25/99-----
12-23 15:20:18 current lr: 0.001
12-23 15:20:19 Epoch: 25 train-Loss: 0.0804 train-Acc: 0.9789, Cost 0.3736 sec
12-23 15:20:19 Epoch: 25 val-Loss: 0.0956 val-Acc: 0.9579, Cost 0.0340 sec
12-23 15:20:19 -----Epoch 26/99-----
12-23 15:20:19 current lr: 0.001
12-23 15:20:19 Epoch: 26 train-Loss: 0.0925 train-Acc: 0.9665, Cost 0.3626 sec
12-23 15:20:19 Epoch: 26 val-Loss: 0.0890 val-Acc: 0.9732, Cost 0.0350 sec
12-23 15:20:19 save best model epoch 26, acc 0.9732
12-23 15:20:19 -----Epoch 27/99-----
12-23 15:20:19 current lr: 0.001
12-23 15:20:19 Epoch: 27 train-Loss: 0.0960 train-Acc: 0.9636, Cost 0.3646 sec
12-23 15:20:19 Epoch: 27 val-Loss: 0.2259 val-Acc: 0.9042, Cost 0.0350 sec
12-23 15:20:19 -----Epoch 28/99-----
12-23 15:20:19 current lr: 0.001
12-23 15:20:20 Epoch: 28 train-Loss: 0.0988 train-Acc: 0.9703, Cost 0.3626 sec
12-23 15:20:20 Epoch: 28 val-Loss: 0.1337 val-Acc: 0.9579, Cost 0.0350 sec
12-23 15:20:20 -----Epoch 29/99-----
12-23 15:20:20 current lr: 0.001
12-23 15:20:20 Epoch: 29 [448/1044], Train Loss: 0.0908 Train Acc: 0.9702,2589.6 examples/sec 0.02 sec/batch
12-23 15:20:20 Epoch: 29 train-Loss: 0.0625 train-Acc: 0.9818, Cost 0.3616 sec
12-23 15:20:20 Epoch: 29 val-Loss: 0.0901 val-Acc: 0.9693, Cost 0.0350 sec
12-23 15:20:20 -----Epoch 30/99-----
12-23 15:20:20 current lr: 0.001
12-23 15:20:21 Epoch: 30 train-Loss: 0.1032 train-Acc: 0.9684, Cost 0.3806 sec
12-23 15:20:21 Epoch: 30 val-Loss: 0.4915 val-Acc: 0.8199, Cost 0.0340 sec
12-23 15:20:21 -----Epoch 31/99-----
12-23 15:20:21 current lr: 0.001
12-23 15:20:21 Epoch: 31 train-Loss: 0.1186 train-Acc: 0.9598, Cost 0.3696 sec
12-23 15:20:21 Epoch: 31 val-Loss: 0.1066 val-Acc: 0.9540, Cost 0.0350 sec
12-23 15:20:21 -----Epoch 32/99-----
12-23 15:20:21 current lr: 0.001
12-23 15:20:21 Epoch: 32 train-Loss: 0.0789 train-Acc: 0.9751, Cost 0.3636 sec
12-23 15:20:21 Epoch: 32 val-Loss: 0.0425 val-Acc: 0.9847, Cost 0.0350 sec
12-23 15:20:21 save best model epoch 32, acc 0.9847
12-23 15:20:21 -----Epoch 33/99-----
12-23 15:20:21 current lr: 0.001
12-23 15:20:22 Epoch: 33 train-Loss: 0.0597 train-Acc: 0.9828, Cost 0.3656 sec
12-23 15:20:22 Epoch: 33 val-Loss: 0.0489 val-Acc: 0.9770, Cost 0.0350 sec
12-23 15:20:22 -----Epoch 34/99-----
12-23 15:20:22 current lr: 0.001
12-23 15:20:22 Epoch: 34 train-Loss: 0.0564 train-Acc: 0.9799, Cost 0.3786 sec
12-23 15:20:22 Epoch: 34 val-Loss: 0.0361 val-Acc: 0.9923, Cost 0.0340 sec
12-23 15:20:22 save best model epoch 34, acc 0.9923
12-23 15:20:22 -----Epoch 35/99-----
12-23 15:20:22 current lr: 0.001
12-23 15:20:22 Epoch: 35 [320/1044], Train Loss: 0.0785 Train Acc: 0.9749,2554.1 examples/sec 0.02 sec/batch
12-23 15:20:23 Epoch: 35 train-Loss: 0.0666 train-Acc: 0.9770, Cost 0.3756 sec
12-23 15:20:23 Epoch: 35 val-Loss: 0.0290 val-Acc: 0.9962, Cost 0.0360 sec
12-23 15:20:23 save best model epoch 35, acc 0.9962
12-23 15:20:23 -----Epoch 36/99-----
12-23 15:20:23 current lr: 0.001
12-23 15:20:23 Epoch: 36 train-Loss: 0.1038 train-Acc: 0.9703, Cost 0.3796 sec
12-23 15:20:23 Epoch: 36 val-Loss: 0.0910 val-Acc: 0.9464, Cost 0.0340 sec
12-23 15:20:23 -----Epoch 37/99-----
12-23 15:20:23 current lr: 0.001
12-23 15:20:23 Epoch: 37 train-Loss: 0.0819 train-Acc: 0.9722, Cost 0.3706 sec
12-23 15:20:24 Epoch: 37 val-Loss: 0.1084 val-Acc: 0.9655, Cost 0.0350 sec
12-23 15:20:24 -----Epoch 38/99-----
12-23 15:20:24 current lr: 0.001
12-23 15:20:24 Epoch: 38 train-Loss: 0.0573 train-Acc: 0.9847, Cost 0.3776 sec
12-23 15:20:24 Epoch: 38 val-Loss: 0.0477 val-Acc: 0.9923, Cost 0.0350 sec
12-23 15:20:24 -----Epoch 39/99-----
12-23 15:20:24 current lr: 0.001
12-23 15:20:24 Epoch: 39 train-Loss: 0.0565 train-Acc: 0.9789, Cost 0.3836 sec
12-23 15:20:24 Epoch: 39 val-Loss: 0.0300 val-Acc: 0.9885, Cost 0.0340 sec
12-23 15:20:24 -----Epoch 40/99-----
12-23 15:20:24 current lr: 0.001
12-23 15:20:25 Epoch: 40 train-Loss: 0.0464 train-Acc: 0.9866, Cost 0.3626 sec
12-23 15:20:25 Epoch: 40 val-Loss: 0.0241 val-Acc: 0.9962, Cost 0.0340 sec
12-23 15:20:25 -----Epoch 41/99-----
12-23 15:20:25 current lr: 0.001
12-23 15:20:25 Epoch: 41 [192/1044], Train Loss: 0.0695 Train Acc: 0.9782,2544.5 examples/sec 0.02 sec/batch
12-23 15:20:25 Epoch: 41 train-Loss: 0.0570 train-Acc: 0.9818, Cost 0.3616 sec
12-23 15:20:25 Epoch: 41 val-Loss: 0.0417 val-Acc: 0.9847, Cost 0.0340 sec
12-23 15:20:25 -----Epoch 42/99-----
12-23 15:20:25 current lr: 0.001
12-23 15:20:26 Epoch: 42 train-Loss: 0.0736 train-Acc: 0.9751, Cost 0.3816 sec
12-23 15:20:26 Epoch: 42 val-Loss: 0.0365 val-Acc: 0.9885, Cost 0.0350 sec
12-23 15:20:26 -----Epoch 43/99-----
12-23 15:20:26 current lr: 0.001
12-23 15:20:26 Epoch: 43 train-Loss: 0.0560 train-Acc: 0.9799, Cost 0.3716 sec
12-23 15:20:26 Epoch: 43 val-Loss: 0.0655 val-Acc: 0.9732, Cost 0.0370 sec
12-23 15:20:26 -----Epoch 44/99-----
12-23 15:20:26 current lr: 0.001
12-23 15:20:26 Epoch: 44 train-Loss: 0.0409 train-Acc: 0.9875, Cost 0.3706 sec
12-23 15:20:26 Epoch: 44 val-Loss: 0.0911 val-Acc: 0.9617, Cost 0.0340 sec
12-23 15:20:26 -----Epoch 45/99-----
12-23 15:20:26 current lr: 0.001
12-23 15:20:27 Epoch: 45 train-Loss: 0.0381 train-Acc: 0.9895, Cost 0.3636 sec
12-23 15:20:27 Epoch: 45 val-Loss: 0.0515 val-Acc: 0.9847, Cost 0.0350 sec
12-23 15:20:27 -----Epoch 46/99-----
12-23 15:20:27 current lr: 0.001
12-23 15:20:27 Epoch: 46 train-Loss: 0.0380 train-Acc: 0.9885, Cost 0.3666 sec
12-23 15:20:27 Epoch: 46 val-Loss: 0.0632 val-Acc: 0.9732, Cost 0.0350 sec
12-23 15:20:27 -----Epoch 47/99-----
12-23 15:20:27 current lr: 0.001
12-23 15:20:27 Epoch: 47 [64/1044], Train Loss: 0.0508 Train Acc: 0.9835,2564.7 examples/sec 0.02 sec/batch
12-23 15:20:28 Epoch: 47 train-Loss: 0.0306 train-Acc: 0.9875, Cost 0.3686 sec
12-23 15:20:28 Epoch: 47 val-Loss: 0.0847 val-Acc: 0.9770, Cost 0.0350 sec
12-23 15:20:28 -----Epoch 48/99-----
12-23 15:20:28 current lr: 0.001
12-23 15:20:28 Epoch: 48 train-Loss: 0.0211 train-Acc: 0.9914, Cost 0.3556 sec
12-23 15:20:28 Epoch: 48 val-Loss: 0.0101 val-Acc: 1.0000, Cost 0.0350 sec
12-23 15:20:28 save best model epoch 48, acc 1.0000
12-23 15:20:28 -----Epoch 49/99-----
12-23 15:20:28 current lr: 0.001
12-23 15:20:28 Epoch: 49 train-Loss: 0.0148 train-Acc: 0.9981, Cost 0.3766 sec
12-23 15:20:28 Epoch: 49 val-Loss: 0.0282 val-Acc: 0.9847, Cost 0.0360 sec
12-23 15:20:28 -----Epoch 50/99-----
12-23 15:20:28 current lr: 0.001
12-23 15:20:29 Epoch: 50 train-Loss: 0.0162 train-Acc: 0.9952, Cost 0.3646 sec
12-23 15:20:29 Epoch: 50 val-Loss: 0.0218 val-Acc: 0.9962, Cost 0.0340 sec
12-23 15:20:29 -----Epoch 51/99-----
12-23 15:20:29 current lr: 0.001
12-23 15:20:29 Epoch: 51 train-Loss: 0.0192 train-Acc: 0.9952, Cost 0.3626 sec
12-23 15:20:29 Epoch: 51 val-Loss: 0.0177 val-Acc: 0.9923, Cost 0.0340 sec
12-23 15:20:29 -----Epoch 52/99-----
12-23 15:20:29 current lr: 0.001
12-23 15:20:30 Epoch: 52 [320/1044], Train Loss: 0.0211 Train Acc: 0.9933,2643.1 examples/sec 0.02 sec/batch
12-23 15:20:30 Epoch: 52 train-Loss: 0.0250 train-Acc: 0.9923, Cost 0.3566 sec
12-23 15:20:30 Epoch: 52 val-Loss: 0.0621 val-Acc: 0.9770, Cost 0.0350 sec
12-23 15:20:30 -----Epoch 53/99-----
12-23 15:20:30 current lr: 0.001
12-23 15:20:30 Epoch: 53 train-Loss: 0.0251 train-Acc: 0.9895, Cost 0.3646 sec
12-23 15:20:30 Epoch: 53 val-Loss: 0.0644 val-Acc: 0.9732, Cost 0.0350 sec
12-23 15:20:30 -----Epoch 54/99-----
12-23 15:20:30 current lr: 0.001
12-23 15:20:30 Epoch: 54 train-Loss: 0.0123 train-Acc: 0.9981, Cost 0.3636 sec
12-23 15:20:30 Epoch: 54 val-Loss: 0.0109 val-Acc: 1.0000, Cost 0.0360 sec
12-23 15:20:30 -----Epoch 55/99-----
12-23 15:20:30 current lr: 0.001
12-23 15:20:31 Epoch: 55 train-Loss: 0.0170 train-Acc: 0.9952, Cost 0.3716 sec
12-23 15:20:31 Epoch: 55 val-Loss: 0.0517 val-Acc: 0.9732, Cost 0.0350 sec
12-23 15:20:31 -----Epoch 56/99-----
12-23 15:20:31 current lr: 0.001
12-23 15:20:31 Epoch: 56 train-Loss: 0.0479 train-Acc: 0.9799, Cost 0.3626 sec
12-23 15:20:31 Epoch: 56 val-Loss: 0.1829 val-Acc: 0.9157, Cost 0.0350 sec
12-23 15:20:31 -----Epoch 57/99-----
12-23 15:20:31 current lr: 0.001
12-23 15:20:32 Epoch: 57 train-Loss: 0.0778 train-Acc: 0.9741, Cost 0.3786 sec
12-23 15:20:32 Epoch: 57 val-Loss: 0.0573 val-Acc: 0.9693, Cost 0.0350 sec
12-23 15:20:32 -----Epoch 58/99-----
12-23 15:20:32 current lr: 0.001
12-23 15:20:32 Epoch: 58 [896/1044], Train Loss: 0.0347 Train Acc: 0.9882,2584.2 examples/sec 0.02 sec/batch
12-23 15:20:32 Epoch: 58 train-Loss: 0.0271 train-Acc: 0.9923, Cost 0.3696 sec
12-23 15:20:32 Epoch: 58 val-Loss: 0.0288 val-Acc: 0.9962, Cost 0.0360 sec
12-23 15:20:32 -----Epoch 59/99-----
12-23 15:20:32 current lr: 0.001
12-23 15:20:32 Epoch: 59 train-Loss: 0.0248 train-Acc: 0.9923, Cost 0.3546 sec
12-23 15:20:32 Epoch: 59 val-Loss: 0.0225 val-Acc: 0.9885, Cost 0.0340 sec
12-23 15:20:32 -----Epoch 60/99-----
12-23 15:20:32 current lr: 0.001
12-23 15:20:33 Epoch: 60 train-Loss: 0.0168 train-Acc: 0.9952, Cost 0.3906 sec
12-23 15:20:33 Epoch: 60 val-Loss: 0.0771 val-Acc: 0.9732, Cost 0.0350 sec
12-23 15:20:33 -----Epoch 61/99-----
12-23 15:20:33 current lr: 0.001
12-23 15:20:33 Epoch: 61 train-Loss: 0.0285 train-Acc: 0.9885, Cost 0.3676 sec
12-23 15:20:33 Epoch: 61 val-Loss: 0.0116 val-Acc: 0.9962, Cost 0.0340 sec
12-23 15:20:33 -----Epoch 62/99-----
12-23 15:20:33 current lr: 0.001
12-23 15:20:34 Epoch: 62 train-Loss: 0.0106 train-Acc: 0.9990, Cost 0.3596 sec
12-23 15:20:34 Epoch: 62 val-Loss: 0.0214 val-Acc: 0.9923, Cost 0.0350 sec
12-23 15:20:34 -----Epoch 63/99-----
12-23 15:20:34 current lr: 0.001
12-23 15:20:34 Epoch: 63 train-Loss: 0.0229 train-Acc: 0.9933, Cost 0.3596 sec
12-23 15:20:34 Epoch: 63 val-Loss: 0.0277 val-Acc: 0.9923, Cost 0.0350 sec
12-23 15:20:34 -----Epoch 64/99-----
12-23 15:20:34 current lr: 0.001
12-23 15:20:34 Epoch: 64 [768/1044], Train Loss: 0.0210 Train Acc: 0.9932,2592.9 examples/sec 0.02 sec/batch
12-23 15:20:34 Epoch: 64 train-Loss: 0.0197 train-Acc: 0.9923, Cost 0.3536 sec
12-23 15:20:34 Epoch: 64 val-Loss: 0.0655 val-Acc: 0.9770, Cost 0.0340 sec
12-23 15:20:34 -----Epoch 65/99-----
12-23 15:20:34 current lr: 0.001
12-23 15:20:35 Epoch: 65 train-Loss: 0.0222 train-Acc: 0.9933, Cost 0.3556 sec
12-23 15:20:35 Epoch: 65 val-Loss: 0.0476 val-Acc: 0.9808, Cost 0.0350 sec
12-23 15:20:35 -----Epoch 66/99-----
12-23 15:20:35 current lr: 0.001
12-23 15:20:35 Epoch: 66 train-Loss: 0.0139 train-Acc: 0.9962, Cost 0.3626 sec
12-23 15:20:35 Epoch: 66 val-Loss: 0.0455 val-Acc: 0.9732, Cost 0.0340 sec
12-23 15:20:35 -----Epoch 67/99-----
12-23 15:20:35 current lr: 0.001
12-23 15:20:36 Epoch: 67 train-Loss: 0.0199 train-Acc: 0.9933, Cost 0.3696 sec
12-23 15:20:36 Epoch: 67 val-Loss: 0.0314 val-Acc: 0.9885, Cost 0.0360 sec
12-23 15:20:36 -----Epoch 68/99-----
12-23 15:20:36 current lr: 0.001
12-23 15:20:36 Epoch: 68 train-Loss: 0.0819 train-Acc: 0.9761, Cost 0.3706 sec
12-23 15:20:36 Epoch: 68 val-Loss: 0.1676 val-Acc: 0.9464, Cost 0.0360 sec
12-23 15:20:36 -----Epoch 69/99-----
12-23 15:20:36 current lr: 0.001
12-23 15:20:36 Epoch: 69 train-Loss: 0.0545 train-Acc: 0.9856, Cost 0.3656 sec
12-23 15:20:36 Epoch: 69 val-Loss: 0.1248 val-Acc: 0.9502, Cost 0.0340 sec
12-23 15:20:36 -----Epoch 70/99-----
12-23 15:20:36 current lr: 0.001
12-23 15:20:37 Epoch: 70 [640/1044], Train Loss: 0.0386 Train Acc: 0.9894,2598.3 examples/sec 0.02 sec/batch
12-23 15:20:37 Epoch: 70 train-Loss: 0.0405 train-Acc: 0.9914, Cost 0.3716 sec
12-23 15:20:37 Epoch: 70 val-Loss: 0.0310 val-Acc: 0.9885, Cost 0.0350 sec
12-23 15:20:37 -----Epoch 71/99-----
12-23 15:20:37 current lr: 0.001
12-23 15:20:37 Epoch: 71 train-Loss: 0.0157 train-Acc: 0.9952, Cost 0.3756 sec
12-23 15:20:37 Epoch: 71 val-Loss: 0.0390 val-Acc: 0.9808, Cost 0.0350 sec
12-23 15:20:37 -----Epoch 72/99-----
12-23 15:20:37 current lr: 0.001
12-23 15:20:38 Epoch: 72 train-Loss: 0.0148 train-Acc: 0.9943, Cost 0.3616 sec
12-23 15:20:38 Epoch: 72 val-Loss: 0.0091 val-Acc: 0.9962, Cost 0.0370 sec
12-23 15:20:38 -----Epoch 73/99-----
12-23 15:20:38 current lr: 0.001
12-23 15:20:38 Epoch: 73 train-Loss: 0.0150 train-Acc: 0.9943, Cost 0.3746 sec
12-23 15:20:38 Epoch: 73 val-Loss: 0.0366 val-Acc: 0.9885, Cost 0.0340 sec
12-23 15:20:38 -----Epoch 74/99-----
12-23 15:20:38 current lr: 0.001
12-23 15:20:38 Epoch: 74 train-Loss: 0.0054 train-Acc: 0.9990, Cost 0.3726 sec
12-23 15:20:38 Epoch: 74 val-Loss: 0.0114 val-Acc: 0.9962, Cost 0.0340 sec
12-23 15:20:38 -----Epoch 75/99-----
12-23 15:20:38 current lr: 0.001
12-23 15:20:39 Epoch: 75 train-Loss: 0.0063 train-Acc: 0.9990, Cost 0.3716 sec
12-23 15:20:39 Epoch: 75 val-Loss: 0.0031 val-Acc: 1.0000, Cost 0.0340 sec
12-23 15:20:39 -----Epoch 76/99-----
12-23 15:20:39 current lr: 0.001
12-23 15:20:39 Epoch: 76 [512/1044], Train Loss: 0.0115 Train Acc: 0.9966,2567.9 examples/sec 0.02 sec/batch
12-23 15:20:39 Epoch: 76 train-Loss: 0.0056 train-Acc: 0.9990, Cost 0.3606 sec
12-23 15:20:39 Epoch: 76 val-Loss: 0.0192 val-Acc: 0.9847, Cost 0.0350 sec
12-23 15:20:39 -----Epoch 77/99-----
12-23 15:20:39 current lr: 0.001
12-23 15:20:40 Epoch: 77 train-Loss: 0.0132 train-Acc: 0.9962, Cost 0.3766 sec
12-23 15:20:40 Epoch: 77 val-Loss: 0.0239 val-Acc: 0.9847, Cost 0.0350 sec
12-23 15:20:40 -----Epoch 78/99-----
12-23 15:20:40 current lr: 0.001
12-23 15:20:40 Epoch: 78 train-Loss: 0.0292 train-Acc: 0.9885, Cost 0.3746 sec
12-23 15:20:40 Epoch: 78 val-Loss: 0.0473 val-Acc: 0.9847, Cost 0.0350 sec
12-23 15:20:40 -----Epoch 79/99-----
12-23 15:20:40 current lr: 0.001
12-23 15:20:40 Epoch: 79 train-Loss: 0.0465 train-Acc: 0.9875, Cost 0.3706 sec
12-23 15:20:40 Epoch: 79 val-Loss: 0.3100 val-Acc: 0.9195, Cost 0.0360 sec
12-23 15:20:40 -----Epoch 80/99-----
12-23 15:20:40 current lr: 0.001
12-23 15:20:41 Epoch: 80 train-Loss: 0.0661 train-Acc: 0.9761, Cost 0.3786 sec
12-23 15:20:41 Epoch: 80 val-Loss: 0.0853 val-Acc: 0.9617, Cost 0.0350 sec
12-23 15:20:41 -----Epoch 81/99-----
12-23 15:20:41 current lr: 0.001
12-23 15:20:41 Epoch: 81 train-Loss: 0.0421 train-Acc: 0.9837, Cost 0.3796 sec
12-23 15:20:41 Epoch: 81 val-Loss: 0.0144 val-Acc: 0.9962, Cost 0.0350 sec
12-23 15:20:41 -----Epoch 82/99-----
12-23 15:20:41 current lr: 0.001
12-23 15:20:41 Epoch: 82 [384/1044], Train Loss: 0.0355 Train Acc: 0.9878,2545.6 examples/sec 0.02 sec/batch
12-23 15:20:42 Epoch: 82 train-Loss: 0.0170 train-Acc: 0.9943, Cost 0.3656 sec
12-23 15:20:42 Epoch: 82 val-Loss: 0.0271 val-Acc: 0.9885, Cost 0.0440 sec
12-23 15:20:42 -----Epoch 83/99-----
12-23 15:20:42 current lr: 0.001
12-23 15:20:42 Epoch: 83 train-Loss: 0.0182 train-Acc: 0.9943, Cost 0.3656 sec
12-23 15:20:42 Epoch: 83 val-Loss: 0.0791 val-Acc: 0.9617, Cost 0.0350 sec
12-23 15:20:42 -----Epoch 84/99-----
12-23 15:20:42 current lr: 0.001
12-23 15:20:42 Epoch: 84 train-Loss: 0.0097 train-Acc: 0.9981, Cost 0.3796 sec
12-23 15:20:43 Epoch: 84 val-Loss: 0.0329 val-Acc: 0.9923, Cost 0.0370 sec
12-23 15:20:43 -----Epoch 85/99-----
12-23 15:20:43 current lr: 0.001
12-23 15:20:43 Epoch: 85 train-Loss: 0.0150 train-Acc: 0.9981, Cost 0.3766 sec
12-23 15:20:43 Epoch: 85 val-Loss: 0.0867 val-Acc: 0.9579, Cost 0.0350 sec
12-23 15:20:43 -----Epoch 86/99-----
12-23 15:20:43 current lr: 0.001
12-23 15:20:43 Epoch: 86 train-Loss: 0.0042 train-Acc: 1.0000, Cost 0.3766 sec
12-23 15:20:43 Epoch: 86 val-Loss: 0.0073 val-Acc: 0.9962, Cost 0.0350 sec
12-23 15:20:43 -----Epoch 87/99-----
12-23 15:20:43 current lr: 0.001
12-23 15:20:44 Epoch: 87 train-Loss: 0.0030 train-Acc: 1.0000, Cost 0.3566 sec
12-23 15:20:44 Epoch: 87 val-Loss: 0.0063 val-Acc: 0.9962, Cost 0.0360 sec
12-23 15:20:44 -----Epoch 88/99-----
12-23 15:20:44 current lr: 0.001
12-23 15:20:44 Epoch: 88 [256/1044], Train Loss: 0.0101 Train Acc: 0.9979,2557.2 examples/sec 0.02 sec/batch
12-23 15:20:44 Epoch: 88 train-Loss: 0.0058 train-Acc: 0.9981, Cost 0.3616 sec
12-23 15:20:44 Epoch: 88 val-Loss: 0.0068 val-Acc: 1.0000, Cost 0.0340 sec
12-23 15:20:44 -----Epoch 89/99-----
12-23 15:20:44 current lr: 0.001
12-23 15:20:45 Epoch: 89 train-Loss: 0.0099 train-Acc: 0.9952, Cost 0.3686 sec
12-23 15:20:45 Epoch: 89 val-Loss: 0.0053 val-Acc: 1.0000, Cost 0.0340 sec
12-23 15:20:45 -----Epoch 90/99-----
12-23 15:20:45 current lr: 0.001
12-23 15:20:45 Epoch: 90 train-Loss: 0.0074 train-Acc: 0.9981, Cost 0.3716 sec
12-23 15:20:45 Epoch: 90 val-Loss: 0.0041 val-Acc: 1.0000, Cost 0.0340 sec
12-23 15:20:45 -----Epoch 91/99-----
12-23 15:20:45 current lr: 0.001
12-23 15:20:45 Epoch: 91 train-Loss: 0.0055 train-Acc: 0.9990, Cost 0.3816 sec
12-23 15:20:45 Epoch: 91 val-Loss: 0.0262 val-Acc: 0.9923, Cost 0.0410 sec
12-23 15:20:45 -----Epoch 92/99-----
12-23 15:20:45 current lr: 0.001
12-23 15:20:46 Epoch: 92 train-Loss: 0.0058 train-Acc: 0.9990, Cost 0.3726 sec
12-23 15:20:46 Epoch: 92 val-Loss: 0.0395 val-Acc: 0.9847, Cost 0.0390 sec
12-23 15:20:46 -----Epoch 93/99-----
12-23 15:20:46 current lr: 0.001
12-23 15:20:46 Epoch: 93 train-Loss: 0.0039 train-Acc: 0.9990, Cost 0.3796 sec
12-23 15:20:46 Epoch: 93 val-Loss: 0.0792 val-Acc: 0.9770, Cost 0.0350 sec
12-23 15:20:46 -----Epoch 94/99-----
12-23 15:20:46 current lr: 0.001
12-23 15:20:46 Epoch: 94 [128/1044], Train Loss: 0.0065 Train Acc: 0.9980,2542.4 examples/sec 0.02 sec/batch
12-23 15:20:47 Epoch: 94 train-Loss: 0.0097 train-Acc: 0.9981, Cost 0.3746 sec
12-23 15:20:47 Epoch: 94 val-Loss: 0.0927 val-Acc: 0.9693, Cost 0.0410 sec
12-23 15:20:47 -----Epoch 95/99-----
12-23 15:20:47 current lr: 0.001
12-23 15:20:47 Epoch: 95 train-Loss: 0.0034 train-Acc: 0.9990, Cost 0.3646 sec
12-23 15:20:47 Epoch: 95 val-Loss: 0.0648 val-Acc: 0.9847, Cost 0.0350 sec
12-23 15:20:47 -----Epoch 96/99-----
12-23 15:20:47 current lr: 0.001
12-23 15:20:47 Epoch: 96 train-Loss: 0.0097 train-Acc: 0.9981, Cost 0.3726 sec
12-23 15:20:47 Epoch: 96 val-Loss: 0.0162 val-Acc: 0.9923, Cost 0.0350 sec
12-23 15:20:47 -----Epoch 97/99-----
12-23 15:20:47 current lr: 0.001
12-23 15:20:48 Epoch: 97 train-Loss: 0.0157 train-Acc: 0.9962, Cost 0.3626 sec
12-23 15:20:48 Epoch: 97 val-Loss: 0.1799 val-Acc: 0.9464, Cost 0.0350 sec
12-23 15:20:48 -----Epoch 98/99-----
12-23 15:20:48 current lr: 0.001
12-23 15:20:48 Epoch: 98 train-Loss: 0.0103 train-Acc: 0.9971, Cost 0.3666 sec
12-23 15:20:48 Epoch: 98 val-Loss: 0.0486 val-Acc: 0.9885, Cost 0.0360 sec
12-23 15:20:48 -----Epoch 99/99-----
12-23 15:20:48 current lr: 0.001
12-23 15:20:49 Epoch: 99 train-Loss: 0.0047 train-Acc: 1.0000, Cost 0.3826 sec
12-23 15:20:49 Epoch: 99 val-Loss: 0.0629 val-Acc: 0.9808, Cost 0.0410 sec
12-23 15:20:49 save best model epoch 99, acc 0.9808
