12-23 20:10:36 model_name: Dae2d
12-23 20:10:36 data_name: CWRUCWT
12-23 20:10:36 data_dir: C:\Users\Tracy_Lucia\Desktop\CWRU
12-23 20:10:36 normlizetype: 0-1
12-23 20:10:36 processing_type: R_A
12-23 20:10:36 cuda_device: 0
12-23 20:10:36 checkpoint_dir: ./checkpoint
12-23 20:10:36 pretrained: True
12-23 20:10:36 batch_size: 32
12-23 20:10:36 num_workers: 0
12-23 20:10:36 opt: adam
12-23 20:10:36 lr: 0.001
12-23 20:10:36 momentum: 0.9
12-23 20:10:36 weight_decay: 1e-05
12-23 20:10:36 lr_scheduler: fix
12-23 20:10:36 gamma: 0.1
12-23 20:10:36 steps: 10,20,30,40
12-23 20:10:36 steps1: 50,80
12-23 20:10:36 middle_epoch: 50
12-23 20:10:36 max_epoch: 100
12-23 20:10:36 print_step: 100
12-23 20:10:36 using 1 cpu
12-23 20:10:43 -----Epoch 0/49-----
12-23 20:10:43 current lr: 0.001
12-23 20:10:43 Epoch: 0 [0/1068], Train Loss: 0.4476198.8 examples/sec 0.16 sec/batch
12-23 20:10:51 Epoch: 0 train-Loss: 0.2199, Cost 8.4123 sec
12-23 20:10:53 Epoch: 0 val-Loss: 0.0768, Cost 1.9122 sec
12-23 20:10:53 -----Epoch 1/49-----
12-23 20:10:53 current lr: 0.001
12-23 20:11:08 Epoch: 1 train-Loss: 0.1128, Cost 14.9639 sec
12-23 20:11:09 Epoch: 1 val-Loss: 0.1102, Cost 0.4160 sec
12-23 20:11:09 -----Epoch 2/49-----
12-23 20:11:09 current lr: 0.001
12-23 20:11:13 Epoch: 2 [1024/1068], Train Loss: 0.1345105.5 examples/sec 0.30 sec/batch
12-23 20:11:13 Epoch: 2 train-Loss: 0.0791, Cost 4.4623 sec
12-23 20:11:13 Epoch: 2 val-Loss: 0.0615, Cost 0.3996 sec
12-23 20:11:13 -----Epoch 3/49-----
12-23 20:11:13 current lr: 0.001
12-23 20:11:18 Epoch: 3 train-Loss: 0.0667, Cost 4.3749 sec
12-23 20:11:18 Epoch: 3 val-Loss: 0.0509, Cost 0.4243 sec
12-23 20:11:18 -----Epoch 4/49-----
12-23 20:11:18 current lr: 0.001
12-23 20:11:23 Epoch: 4 train-Loss: 0.0546, Cost 4.4036 sec
12-23 20:11:23 Epoch: 4 val-Loss: 0.0404, Cost 0.4129 sec
12-23 20:11:23 -----Epoch 5/49-----
12-23 20:11:23 current lr: 0.001
12-23 20:11:27 Epoch: 5 [960/1068], Train Loss: 0.0564221.2 examples/sec 0.14 sec/batch
12-23 20:11:28 Epoch: 5 train-Loss: 0.0475, Cost 4.4544 sec
12-23 20:11:28 Epoch: 5 val-Loss: 0.0340, Cost 0.4025 sec
12-23 20:11:28 -----Epoch 6/49-----
12-23 20:11:28 current lr: 0.001
12-23 20:11:33 Epoch: 6 train-Loss: 0.0370, Cost 4.7082 sec
12-23 20:11:33 Epoch: 6 val-Loss: 0.0380, Cost 0.4733 sec
12-23 20:11:33 -----Epoch 7/49-----
12-23 20:11:33 current lr: 0.001
12-23 20:11:38 Epoch: 7 train-Loss: 0.0310, Cost 4.8559 sec
12-23 20:11:38 Epoch: 7 val-Loss: 0.0185, Cost 0.4355 sec
12-23 20:11:38 -----Epoch 8/49-----
12-23 20:11:38 current lr: 0.001
12-23 20:11:42 Epoch: 8 [896/1068], Train Loss: 0.0330206.1 examples/sec 0.15 sec/batch
12-23 20:11:43 Epoch: 8 train-Loss: 0.0292, Cost 4.6197 sec
12-23 20:11:43 Epoch: 8 val-Loss: 0.0182, Cost 0.4211 sec
12-23 20:11:43 -----Epoch 9/49-----
12-23 20:11:43 current lr: 0.001
12-23 20:11:48 Epoch: 9 train-Loss: 0.0235, Cost 4.4628 sec
12-23 20:11:48 Epoch: 9 val-Loss: 0.0168, Cost 0.4341 sec
12-23 20:11:48 -----Epoch 10/49-----
12-23 20:11:48 current lr: 0.001
12-23 20:11:53 Epoch: 10 train-Loss: 0.0206, Cost 4.4719 sec
12-23 20:11:53 Epoch: 10 val-Loss: 0.0196, Cost 0.4103 sec
12-23 20:11:53 -----Epoch 11/49-----
12-23 20:11:53 current lr: 0.001
12-23 20:11:57 Epoch: 11 [832/1068], Train Loss: 0.0214219.5 examples/sec 0.14 sec/batch
12-23 20:11:58 Epoch: 11 train-Loss: 0.0188, Cost 4.3555 sec
12-23 20:11:58 Epoch: 11 val-Loss: 0.0107, Cost 0.4027 sec
12-23 20:11:58 -----Epoch 12/49-----
12-23 20:11:58 current lr: 0.001
12-23 20:12:02 Epoch: 12 train-Loss: 0.0169, Cost 4.4016 sec
12-23 20:12:03 Epoch: 12 val-Loss: 0.0131, Cost 0.4220 sec
12-23 20:12:03 -----Epoch 13/49-----
12-23 20:12:03 current lr: 0.001
12-23 20:12:07 Epoch: 13 train-Loss: 0.0149, Cost 4.4779 sec
12-23 20:12:08 Epoch: 13 val-Loss: 0.0151, Cost 0.4256 sec
12-23 20:12:08 -----Epoch 14/49-----
12-23 20:12:08 current lr: 0.001
12-23 20:12:11 Epoch: 14 [768/1068], Train Loss: 0.0156218.6 examples/sec 0.14 sec/batch
12-23 20:12:12 Epoch: 14 train-Loss: 0.0137, Cost 4.5265 sec
12-23 20:12:13 Epoch: 14 val-Loss: 0.0068, Cost 0.4181 sec
12-23 20:12:13 -----Epoch 15/49-----
12-23 20:12:13 current lr: 0.001
12-23 20:12:17 Epoch: 15 train-Loss: 0.0133, Cost 4.3922 sec
12-23 20:12:18 Epoch: 15 val-Loss: 0.0091, Cost 0.4542 sec
12-23 20:12:18 -----Epoch 16/49-----
12-23 20:12:18 current lr: 0.001
12-23 20:12:25 Epoch: 16 train-Loss: 0.0120, Cost 7.5035 sec
12-23 20:12:26 Epoch: 16 val-Loss: 0.0065, Cost 0.7073 sec
12-23 20:12:26 -----Epoch 17/49-----
12-23 20:12:26 current lr: 0.001
12-23 20:12:31 Epoch: 17 [704/1068], Train Loss: 0.0125158.7 examples/sec 0.20 sec/batch
12-23 20:12:33 Epoch: 17 train-Loss: 0.0120, Cost 7.6402 sec
12-23 20:12:34 Epoch: 17 val-Loss: 0.0092, Cost 0.7574 sec
12-23 20:12:34 -----Epoch 18/49-----
12-23 20:12:34 current lr: 0.001
12-23 20:12:42 Epoch: 18 train-Loss: 0.0113, Cost 7.4618 sec
12-23 20:12:42 Epoch: 18 val-Loss: 0.0099, Cost 0.6552 sec
12-23 20:12:42 -----Epoch 19/49-----
12-23 20:12:42 current lr: 0.001
12-23 20:12:49 Epoch: 19 train-Loss: 0.0100, Cost 7.1343 sec
12-23 20:12:50 Epoch: 19 val-Loss: 0.0078, Cost 0.6830 sec
12-23 20:12:50 -----Epoch 20/49-----
12-23 20:12:50 current lr: 0.001
12-23 20:12:55 Epoch: 20 [640/1068], Train Loss: 0.0106131.5 examples/sec 0.24 sec/batch
12-23 20:12:57 Epoch: 20 train-Loss: 0.0093, Cost 7.3993 sec
12-23 20:12:58 Epoch: 20 val-Loss: 0.0056, Cost 0.6763 sec
12-23 20:12:58 -----Epoch 21/49-----
12-23 20:12:58 current lr: 0.001
12-23 20:13:05 Epoch: 21 train-Loss: 0.0092, Cost 7.0308 sec
12-23 20:13:06 Epoch: 21 val-Loss: 0.0066, Cost 0.6869 sec
12-23 20:13:06 -----Epoch 22/49-----
12-23 20:13:06 current lr: 0.001
12-23 20:13:13 Epoch: 22 train-Loss: 0.0088, Cost 7.3408 sec
12-23 20:13:14 Epoch: 22 val-Loss: 0.0095, Cost 0.6623 sec
12-23 20:13:14 -----Epoch 23/49-----
12-23 20:13:14 current lr: 0.001
12-23 20:13:18 Epoch: 23 [576/1068], Train Loss: 0.0088135.4 examples/sec 0.23 sec/batch
12-23 20:13:21 Epoch: 23 train-Loss: 0.0084, Cost 7.2213 sec
12-23 20:13:22 Epoch: 23 val-Loss: 0.0058, Cost 0.6340 sec
12-23 20:13:22 -----Epoch 24/49-----
12-23 20:13:22 current lr: 0.001
12-23 20:13:29 Epoch: 24 train-Loss: 0.0073, Cost 7.1900 sec
12-23 20:13:30 Epoch: 24 val-Loss: 0.0055, Cost 0.6382 sec
12-23 20:13:30 -----Epoch 25/49-----
12-23 20:13:30 current lr: 0.001
12-23 20:13:37 Epoch: 25 train-Loss: 0.0081, Cost 7.7563 sec
12-23 20:13:38 Epoch: 25 val-Loss: 0.0051, Cost 0.6411 sec
12-23 20:13:38 -----Epoch 26/49-----
12-23 20:13:38 current lr: 0.001
12-23 20:13:42 Epoch: 26 [512/1068], Train Loss: 0.0078132.9 examples/sec 0.24 sec/batch
12-23 20:13:45 Epoch: 26 train-Loss: 0.0075, Cost 7.3716 sec
12-23 20:13:46 Epoch: 26 val-Loss: 0.0046, Cost 0.6554 sec
12-23 20:13:46 -----Epoch 27/49-----
12-23 20:13:46 current lr: 0.001
12-23 20:13:54 Epoch: 27 train-Loss: 0.0071, Cost 8.0465 sec
12-23 20:13:55 Epoch: 27 val-Loss: 0.0050, Cost 0.7835 sec
12-23 20:13:55 -----Epoch 28/49-----
12-23 20:13:55 current lr: 0.001
12-23 20:14:03 Epoch: 28 train-Loss: 0.0077, Cost 7.9850 sec
12-23 20:14:04 Epoch: 28 val-Loss: 0.0040, Cost 0.7143 sec
12-23 20:14:04 -----Epoch 29/49-----
12-23 20:14:04 current lr: 0.001
12-23 20:14:07 Epoch: 29 [448/1068], Train Loss: 0.0074124.4 examples/sec 0.25 sec/batch
12-23 20:14:11 Epoch: 29 train-Loss: 0.0073, Cost 7.4396 sec
12-23 20:14:12 Epoch: 29 val-Loss: 0.0042, Cost 0.6113 sec
12-23 20:14:12 -----Epoch 30/49-----
12-23 20:14:12 current lr: 0.001
12-23 20:14:19 Epoch: 30 train-Loss: 0.0076, Cost 7.2518 sec
12-23 20:14:19 Epoch: 30 val-Loss: 0.0048, Cost 0.4923 sec
12-23 20:14:19 -----Epoch 31/49-----
12-23 20:14:19 current lr: 0.001
12-23 20:14:24 Epoch: 31 train-Loss: 0.0066, Cost 4.8887 sec
12-23 20:14:25 Epoch: 31 val-Loss: 0.0070, Cost 0.4927 sec
12-23 20:14:25 -----Epoch 32/49-----
12-23 20:14:25 current lr: 0.001
12-23 20:14:27 Epoch: 32 [384/1068], Train Loss: 0.0073158.9 examples/sec 0.20 sec/batch
12-23 20:14:30 Epoch: 32 train-Loss: 0.0077, Cost 4.9908 sec
12-23 20:14:30 Epoch: 32 val-Loss: 0.0048, Cost 0.5008 sec
12-23 20:14:30 -----Epoch 33/49-----
12-23 20:14:30 current lr: 0.001
12-23 20:14:35 Epoch: 33 train-Loss: 0.0066, Cost 5.1243 sec
12-23 20:14:36 Epoch: 33 val-Loss: 0.0043, Cost 0.4339 sec
12-23 20:14:36 -----Epoch 34/49-----
12-23 20:14:36 current lr: 0.001
12-23 20:14:41 Epoch: 34 train-Loss: 0.0058, Cost 4.8687 sec
12-23 20:14:41 Epoch: 34 val-Loss: 0.0038, Cost 0.4209 sec
12-23 20:14:41 -----Epoch 35/49-----
12-23 20:14:41 current lr: 0.001
12-23 20:14:43 Epoch: 35 [320/1068], Train Loss: 0.0065195.4 examples/sec 0.16 sec/batch
12-23 20:14:46 Epoch: 35 train-Loss: 0.0068, Cost 4.9551 sec
12-23 20:14:46 Epoch: 35 val-Loss: 0.0075, Cost 0.4198 sec
12-23 20:14:46 -----Epoch 36/49-----
12-23 20:14:46 current lr: 0.001
12-23 20:14:51 Epoch: 36 train-Loss: 0.0073, Cost 5.0112 sec
12-23 20:14:52 Epoch: 36 val-Loss: 0.0043, Cost 0.4423 sec
12-23 20:14:52 -----Epoch 37/49-----
12-23 20:14:52 current lr: 0.001
12-23 20:14:57 Epoch: 37 train-Loss: 0.0086, Cost 5.0508 sec
12-23 20:14:57 Epoch: 37 val-Loss: 0.0101, Cost 0.4389 sec
12-23 20:14:57 -----Epoch 38/49-----
12-23 20:14:57 current lr: 0.001
12-23 20:14:59 Epoch: 38 [256/1068], Train Loss: 0.0080196.3 examples/sec 0.16 sec/batch
12-23 20:15:02 Epoch: 38 train-Loss: 0.0096, Cost 5.1232 sec
12-23 20:15:03 Epoch: 38 val-Loss: 0.0050, Cost 0.4836 sec
12-23 20:15:03 -----Epoch 39/49-----
12-23 20:15:03 current lr: 0.001
12-23 20:15:08 Epoch: 39 train-Loss: 0.0064, Cost 5.1849 sec
12-23 20:15:09 Epoch: 39 val-Loss: 0.0036, Cost 0.4834 sec
12-23 20:15:09 -----Epoch 40/49-----
12-23 20:15:09 current lr: 0.001
12-23 20:15:14 Epoch: 40 train-Loss: 0.0060, Cost 5.2619 sec
12-23 20:15:14 Epoch: 40 val-Loss: 0.0041, Cost 0.4456 sec
12-23 20:15:14 -----Epoch 41/49-----
12-23 20:15:14 current lr: 0.001
12-23 20:15:15 Epoch: 41 [192/1068], Train Loss: 0.0068187.8 examples/sec 0.17 sec/batch
12-23 20:15:19 Epoch: 41 train-Loss: 0.0054, Cost 5.1116 sec
12-23 20:15:20 Epoch: 41 val-Loss: 0.0038, Cost 0.4515 sec
12-23 20:15:20 -----Epoch 42/49-----
12-23 20:15:20 current lr: 0.001
12-23 20:15:25 Epoch: 42 train-Loss: 0.0056, Cost 5.2015 sec
12-23 20:15:26 Epoch: 42 val-Loss: 0.0036, Cost 0.4419 sec
12-23 20:15:26 -----Epoch 43/49-----
12-23 20:15:26 current lr: 0.001
12-23 20:15:31 Epoch: 43 train-Loss: 0.0052, Cost 5.1208 sec
12-23 20:15:31 Epoch: 43 val-Loss: 0.0036, Cost 0.6468 sec
12-23 20:15:31 -----Epoch 44/49-----
12-23 20:15:31 current lr: 0.001
12-23 20:15:32 Epoch: 44 [128/1068], Train Loss: 0.0054184.1 examples/sec 0.17 sec/batch
12-23 20:15:39 Epoch: 44 train-Loss: 0.0052, Cost 8.1276 sec
12-23 20:15:40 Epoch: 44 val-Loss: 0.0034, Cost 0.8219 sec
12-23 20:15:40 -----Epoch 45/49-----
12-23 20:15:40 current lr: 0.001
12-23 20:15:49 Epoch: 45 train-Loss: 0.0051, Cost 8.2920 sec
12-23 20:15:49 Epoch: 45 val-Loss: 0.0033, Cost 0.8556 sec
12-23 20:15:49 -----Epoch 46/49-----
12-23 20:15:49 current lr: 0.001
12-23 20:15:59 Epoch: 46 train-Loss: 0.0051, Cost 9.3495 sec
12-23 20:16:00 Epoch: 46 val-Loss: 0.0036, Cost 0.9602 sec
12-23 20:16:00 -----Epoch 47/49-----
12-23 20:16:00 current lr: 0.001
12-23 20:16:01 Epoch: 47 [64/1068], Train Loss: 0.0051110.8 examples/sec 0.28 sec/batch
12-23 20:16:10 Epoch: 47 train-Loss: 0.0050, Cost 9.8397 sec
12-23 20:16:10 Epoch: 47 val-Loss: 0.0034, Cost 0.8298 sec
12-23 20:16:10 -----Epoch 48/49-----
12-23 20:16:10 current lr: 0.001
12-23 20:16:20 Epoch: 48 train-Loss: 0.0059, Cost 9.6672 sec
12-23 20:16:21 Epoch: 48 val-Loss: 0.0033, Cost 1.0095 sec
12-23 20:16:21 -----Epoch 49/49-----
12-23 20:16:21 current lr: 0.001
12-23 20:16:30 Epoch: 49 train-Loss: 0.0051, Cost 8.5759 sec
12-23 20:16:30 Epoch: 49 val-Loss: 0.0027, Cost 0.8228 sec
12-23 20:16:30 -----Epoch 0/99-----
12-23 20:16:30 current lr: 0.001
12-23 20:16:31 Epoch: 0 [0/1068], Train Loss: 0.0751 Train Acc: 0.0003,105.4 examples/sec 0.30 sec/batch
12-23 20:16:33 Epoch: 0 train-Loss: 2.6571 train-Acc: 0.1798, Cost 2.4948 sec
12-23 20:16:33 Epoch: 0 val-Loss: 2.1630 val-Acc: 0.1873, Cost 0.2308 sec
12-23 20:16:33 save best model epoch 0, acc 0.1873
12-23 20:16:33 -----Epoch 1/99-----
12-23 20:16:33 current lr: 0.001
12-23 20:16:35 Epoch: 1 train-Loss: 2.1150 train-Acc: 0.2172, Cost 2.1762 sec
12-23 20:16:36 Epoch: 1 val-Loss: 2.0178 val-Acc: 0.2472, Cost 0.2316 sec
12-23 20:16:36 save best model epoch 1, acc 0.2472
12-23 20:16:36 -----Epoch 2/99-----
12-23 20:16:36 current lr: 0.001
12-23 20:16:38 Epoch: 2 [1024/1068], Train Loss: 2.2164 Train Acc: 0.2066,440.7 examples/sec 0.07 sec/batch
12-23 20:16:38 Epoch: 2 train-Loss: 2.0161 train-Acc: 0.2182, Cost 2.1689 sec
12-23 20:16:38 Epoch: 2 val-Loss: 2.0085 val-Acc: 0.2509, Cost 0.2436 sec
12-23 20:16:38 save best model epoch 2, acc 0.2509
12-23 20:16:38 -----Epoch 3/99-----
12-23 20:16:38 current lr: 0.001
12-23 20:16:40 Epoch: 3 train-Loss: 1.9709 train-Acc: 0.2481, Cost 2.2957 sec
12-23 20:16:41 Epoch: 3 val-Loss: 1.9424 val-Acc: 0.2959, Cost 0.2497 sec
12-23 20:16:41 save best model epoch 3, acc 0.2959
12-23 20:16:41 -----Epoch 4/99-----
12-23 20:16:41 current lr: 0.001
12-23 20:16:43 Epoch: 4 train-Loss: 1.9129 train-Acc: 0.2725, Cost 2.5205 sec
12-23 20:16:43 Epoch: 4 val-Loss: 1.8676 val-Acc: 0.3258, Cost 0.3040 sec
12-23 20:16:43 save best model epoch 4, acc 0.3258
12-23 20:16:43 -----Epoch 5/99-----
12-23 20:16:43 current lr: 0.001
12-23 20:16:46 Epoch: 5 [960/1068], Train Loss: 1.9130 Train Acc: 0.2701,388.5 examples/sec 0.08 sec/batch
12-23 20:16:46 Epoch: 5 train-Loss: 1.8502 train-Acc: 0.2959, Cost 2.6127 sec
12-23 20:16:46 Epoch: 5 val-Loss: 1.8306 val-Acc: 0.2996, Cost 0.2563 sec
12-23 20:16:46 -----Epoch 6/99-----
12-23 20:16:46 current lr: 0.001
12-23 20:16:48 Epoch: 6 train-Loss: 1.7767 train-Acc: 0.3333, Cost 2.1955 sec
12-23 20:16:49 Epoch: 6 val-Loss: 1.7464 val-Acc: 0.3071, Cost 0.2528 sec
12-23 20:16:49 -----Epoch 7/99-----
12-23 20:16:49 current lr: 0.001
12-23 20:16:51 Epoch: 7 train-Loss: 1.7226 train-Acc: 0.3287, Cost 2.5576 sec
12-23 20:16:52 Epoch: 7 val-Loss: 1.6828 val-Acc: 0.3371, Cost 0.2603 sec
12-23 20:16:52 save best model epoch 7, acc 0.3371
12-23 20:16:52 -----Epoch 8/99-----
12-23 20:16:52 current lr: 0.001
12-23 20:16:54 Epoch: 8 [896/1068], Train Loss: 1.7178 Train Acc: 0.3264,406.8 examples/sec 0.08 sec/batch
12-23 20:16:54 Epoch: 8 train-Loss: 1.6364 train-Acc: 0.3071, Cost 2.3002 sec
12-23 20:16:54 Epoch: 8 val-Loss: 1.6431 val-Acc: 0.3258, Cost 0.2400 sec
12-23 20:16:54 -----Epoch 9/99-----
12-23 20:16:54 current lr: 0.001
12-23 20:16:56 Epoch: 9 train-Loss: 1.5980 train-Acc: 0.3315, Cost 2.1847 sec
12-23 20:16:56 Epoch: 9 val-Loss: 1.5798 val-Acc: 0.3596, Cost 0.2151 sec
12-23 20:16:56 save best model epoch 9, acc 0.3596
12-23 20:16:56 -----Epoch 10/99-----
12-23 20:16:56 current lr: 0.001
12-23 20:16:59 Epoch: 10 train-Loss: 1.5521 train-Acc: 0.3558, Cost 2.2320 sec
12-23 20:16:59 Epoch: 10 val-Loss: 1.5680 val-Acc: 0.3371, Cost 0.2207 sec
12-23 20:16:59 -----Epoch 11/99-----
12-23 20:16:59 current lr: 0.001
12-23 20:17:01 Epoch: 11 [832/1068], Train Loss: 1.5627 Train Acc: 0.3433,436.5 examples/sec 0.07 sec/batch
12-23 20:17:01 Epoch: 11 train-Loss: 1.5170 train-Acc: 0.3567, Cost 2.2085 sec
12-23 20:17:01 Epoch: 11 val-Loss: 1.5212 val-Acc: 0.3708, Cost 0.2344 sec
12-23 20:17:01 save best model epoch 11, acc 0.3708
12-23 20:17:01 -----Epoch 12/99-----
12-23 20:17:01 current lr: 0.001
12-23 20:17:04 Epoch: 12 train-Loss: 1.4754 train-Acc: 0.3689, Cost 2.3223 sec
12-23 20:17:04 Epoch: 12 val-Loss: 1.4774 val-Acc: 0.3820, Cost 0.2331 sec
12-23 20:17:04 save best model epoch 12, acc 0.3820
12-23 20:17:04 -----Epoch 13/99-----
12-23 20:17:04 current lr: 0.001
12-23 20:17:06 Epoch: 13 train-Loss: 1.4764 train-Acc: 0.3848, Cost 2.1988 sec
12-23 20:17:06 Epoch: 13 val-Loss: 1.4280 val-Acc: 0.4831, Cost 0.2211 sec
12-23 20:17:06 save best model epoch 13, acc 0.4831
12-23 20:17:06 -----Epoch 14/99-----
12-23 20:17:06 current lr: 0.001
12-23 20:17:08 Epoch: 14 [768/1068], Train Loss: 1.4548 Train Acc: 0.3898,430.4 examples/sec 0.07 sec/batch
12-23 20:17:09 Epoch: 14 train-Loss: 1.3688 train-Acc: 0.4410, Cost 2.2472 sec
12-23 20:17:09 Epoch: 14 val-Loss: 4.8276 val-Acc: 0.2584, Cost 0.2324 sec
12-23 20:17:09 -----Epoch 15/99-----
12-23 20:17:09 current lr: 0.001
12-23 20:17:11 Epoch: 15 train-Loss: 1.2759 train-Acc: 0.4373, Cost 2.2019 sec
12-23 20:17:11 Epoch: 15 val-Loss: 1.3072 val-Acc: 0.4494, Cost 0.2456 sec
12-23 20:17:11 -----Epoch 16/99-----
12-23 20:17:11 current lr: 0.001
12-23 20:17:14 Epoch: 16 train-Loss: 1.2235 train-Acc: 0.4888, Cost 2.2735 sec
12-23 20:17:14 Epoch: 16 val-Loss: 1.2455 val-Acc: 0.4494, Cost 0.2407 sec
12-23 20:17:14 -----Epoch 17/99-----
12-23 20:17:14 current lr: 0.001
12-23 20:17:15 Epoch: 17 [704/1068], Train Loss: 1.2451 Train Acc: 0.4732,430.0 examples/sec 0.07 sec/batch
12-23 20:17:16 Epoch: 17 train-Loss: 1.1781 train-Acc: 0.4972, Cost 2.2334 sec
12-23 20:17:16 Epoch: 17 val-Loss: 1.3539 val-Acc: 0.5094, Cost 0.2492 sec
12-23 20:17:16 save best model epoch 17, acc 0.5094
12-23 20:17:16 -----Epoch 18/99-----
12-23 20:17:16 current lr: 0.001
12-23 20:17:19 Epoch: 18 train-Loss: 1.0793 train-Acc: 0.5618, Cost 2.3484 sec
12-23 20:17:19 Epoch: 18 val-Loss: 1.2404 val-Acc: 0.6030, Cost 0.2501 sec
12-23 20:17:19 save best model epoch 18, acc 0.6030
12-23 20:17:19 -----Epoch 19/99-----
12-23 20:17:19 current lr: 0.001
12-23 20:17:21 Epoch: 19 train-Loss: 1.0213 train-Acc: 0.5899, Cost 2.3670 sec
12-23 20:17:22 Epoch: 19 val-Loss: 1.3959 val-Acc: 0.5356, Cost 0.2589 sec
12-23 20:17:22 -----Epoch 20/99-----
12-23 20:17:22 current lr: 0.001
12-23 20:17:23 Epoch: 20 [640/1068], Train Loss: 1.0464 Train Acc: 0.5678,413.2 examples/sec 0.08 sec/batch
12-23 20:17:24 Epoch: 20 train-Loss: 0.9668 train-Acc: 0.6114, Cost 2.2442 sec
12-23 20:17:24 Epoch: 20 val-Loss: 1.9052 val-Acc: 0.4494, Cost 0.2326 sec
12-23 20:17:24 -----Epoch 21/99-----
12-23 20:17:24 current lr: 0.001
12-23 20:17:26 Epoch: 21 train-Loss: 0.8438 train-Acc: 0.6554, Cost 2.2147 sec
12-23 20:17:26 Epoch: 21 val-Loss: 4.2837 val-Acc: 0.3783, Cost 0.2212 sec
12-23 20:17:26 -----Epoch 22/99-----
12-23 20:17:26 current lr: 0.001
12-23 20:17:29 Epoch: 22 train-Loss: 0.8268 train-Acc: 0.6358, Cost 2.2559 sec
12-23 20:17:29 Epoch: 22 val-Loss: 1.0265 val-Acc: 0.7041, Cost 0.2309 sec
12-23 20:17:29 save best model epoch 22, acc 0.7041
12-23 20:17:29 -----Epoch 23/99-----
12-23 20:17:29 current lr: 0.001
12-23 20:17:30 Epoch: 23 [576/1068], Train Loss: 0.8338 Train Acc: 0.6481,434.0 examples/sec 0.07 sec/batch
12-23 20:17:31 Epoch: 23 train-Loss: 0.7401 train-Acc: 0.6723, Cost 2.2067 sec
12-23 20:17:31 Epoch: 23 val-Loss: 0.9236 val-Acc: 0.5918, Cost 0.2346 sec
12-23 20:17:31 -----Epoch 24/99-----
12-23 20:17:31 current lr: 0.001
12-23 20:17:34 Epoch: 24 train-Loss: 0.7031 train-Acc: 0.6788, Cost 2.2309 sec
12-23 20:17:34 Epoch: 24 val-Loss: 1.0610 val-Acc: 0.6142, Cost 0.2311 sec
12-23 20:17:34 -----Epoch 25/99-----
12-23 20:17:34 current lr: 0.001
12-23 20:17:36 Epoch: 25 train-Loss: 0.6527 train-Acc: 0.6985, Cost 2.3133 sec
12-23 20:17:36 Epoch: 25 val-Loss: 0.9219 val-Acc: 0.6292, Cost 0.2511 sec
12-23 20:17:36 -----Epoch 26/99-----
12-23 20:17:36 current lr: 0.001
12-23 20:17:38 Epoch: 26 [512/1068], Train Loss: 0.6661 Train Acc: 0.6936,427.7 examples/sec 0.07 sec/batch
12-23 20:17:39 Epoch: 26 train-Loss: 0.5888 train-Acc: 0.7154, Cost 2.1869 sec
12-23 20:17:39 Epoch: 26 val-Loss: 0.9952 val-Acc: 0.6330, Cost 0.2364 sec
12-23 20:17:39 -----Epoch 27/99-----
12-23 20:17:39 current lr: 0.001
12-23 20:17:41 Epoch: 27 train-Loss: 0.6404 train-Acc: 0.7275, Cost 2.2218 sec
12-23 20:17:41 Epoch: 27 val-Loss: 3.9089 val-Acc: 0.4045, Cost 0.2358 sec
12-23 20:17:41 -----Epoch 28/99-----
12-23 20:17:41 current lr: 0.001
12-23 20:17:43 Epoch: 28 train-Loss: 0.6019 train-Acc: 0.7238, Cost 2.2249 sec
12-23 20:17:44 Epoch: 28 val-Loss: 0.9458 val-Acc: 0.6367, Cost 0.2208 sec
12-23 20:17:44 -----Epoch 29/99-----
12-23 20:17:44 current lr: 0.001
12-23 20:17:45 Epoch: 29 [448/1068], Train Loss: 0.6006 Train Acc: 0.7293,435.4 examples/sec 0.07 sec/batch
12-23 20:17:46 Epoch: 29 train-Loss: 0.5708 train-Acc: 0.7275, Cost 2.2638 sec
12-23 20:17:46 Epoch: 29 val-Loss: 0.9939 val-Acc: 0.6779, Cost 0.2420 sec
12-23 20:17:46 -----Epoch 30/99-----
12-23 20:17:46 current lr: 0.001
12-23 20:17:49 Epoch: 30 train-Loss: 0.5655 train-Acc: 0.7584, Cost 2.3083 sec
12-23 20:17:49 Epoch: 30 val-Loss: 2.3629 val-Acc: 0.4944, Cost 0.2494 sec
12-23 20:17:49 -----Epoch 31/99-----
12-23 20:17:49 current lr: 0.001
12-23 20:17:51 Epoch: 31 train-Loss: 0.5004 train-Acc: 0.7753, Cost 2.4354 sec
12-23 20:17:51 Epoch: 31 val-Loss: 0.8968 val-Acc: 0.7041, Cost 0.2607 sec
12-23 20:17:51 -----Epoch 32/99-----
12-23 20:17:51 current lr: 0.001
12-23 20:17:52 Epoch: 32 [384/1068], Train Loss: 0.5434 Train Acc: 0.7599,411.5 examples/sec 0.08 sec/batch
12-23 20:17:54 Epoch: 32 train-Loss: 0.5011 train-Acc: 0.7697, Cost 2.3394 sec
12-23 20:17:54 Epoch: 32 val-Loss: 0.6933 val-Acc: 0.7528, Cost 0.2378 sec
12-23 20:17:54 save best model epoch 32, acc 0.7528
12-23 20:17:54 -----Epoch 33/99-----
12-23 20:17:54 current lr: 0.001
12-23 20:17:56 Epoch: 33 train-Loss: 0.4903 train-Acc: 0.8034, Cost 2.3683 sec
12-23 20:17:57 Epoch: 33 val-Loss: 3.7652 val-Acc: 0.3745, Cost 0.2487 sec
12-23 20:17:57 -----Epoch 34/99-----
12-23 20:17:57 current lr: 0.001
12-23 20:17:59 Epoch: 34 train-Loss: 0.4661 train-Acc: 0.8230, Cost 2.4176 sec
12-23 20:17:59 Epoch: 34 val-Loss: 1.7437 val-Acc: 0.6142, Cost 0.2530 sec
12-23 20:17:59 -----Epoch 35/99-----
12-23 20:17:59 current lr: 0.001
12-23 20:18:00 Epoch: 35 [320/1068], Train Loss: 0.4782 Train Acc: 0.8029,403.4 examples/sec 0.08 sec/batch
12-23 20:18:02 Epoch: 35 train-Loss: 0.4602 train-Acc: 0.8343, Cost 2.5005 sec
12-23 20:18:02 Epoch: 35 val-Loss: 5.2105 val-Acc: 0.3034, Cost 0.2408 sec
12-23 20:18:02 -----Epoch 36/99-----
12-23 20:18:02 current lr: 0.001
12-23 20:18:05 Epoch: 36 train-Loss: 0.3411 train-Acc: 0.8801, Cost 2.4657 sec
12-23 20:18:05 Epoch: 36 val-Loss: 0.8393 val-Acc: 0.7715, Cost 0.2429 sec
12-23 20:18:05 save best model epoch 36, acc 0.7715
12-23 20:18:05 -----Epoch 37/99-----
12-23 20:18:05 current lr: 0.001
12-23 20:18:07 Epoch: 37 train-Loss: 0.3161 train-Acc: 0.8858, Cost 2.4276 sec
12-23 20:18:07 Epoch: 37 val-Loss: 0.6376 val-Acc: 0.7865, Cost 0.2425 sec
12-23 20:18:07 save best model epoch 37, acc 0.7865
12-23 20:18:07 -----Epoch 38/99-----
12-23 20:18:07 current lr: 0.001
12-23 20:18:08 Epoch: 38 [256/1068], Train Loss: 0.3564 Train Acc: 0.8742,394.6 examples/sec 0.08 sec/batch
12-23 20:18:10 Epoch: 38 train-Loss: 0.2617 train-Acc: 0.9148, Cost 2.5127 sec
12-23 20:18:10 Epoch: 38 val-Loss: 0.8163 val-Acc: 0.7940, Cost 0.2620 sec
12-23 20:18:10 save best model epoch 38, acc 0.7940
12-23 20:18:10 -----Epoch 39/99-----
12-23 20:18:10 current lr: 0.001
12-23 20:18:13 Epoch: 39 train-Loss: 0.2435 train-Acc: 0.9064, Cost 2.8633 sec
12-23 20:18:13 Epoch: 39 val-Loss: 0.8091 val-Acc: 0.7828, Cost 0.2613 sec
12-23 20:18:13 -----Epoch 40/99-----
12-23 20:18:13 current lr: 0.001
12-23 20:18:16 Epoch: 40 train-Loss: 0.2053 train-Acc: 0.9326, Cost 2.6268 sec
12-23 20:18:16 Epoch: 40 val-Loss: 0.9792 val-Acc: 0.7491, Cost 0.2576 sec
12-23 20:18:16 -----Epoch 41/99-----
12-23 20:18:16 current lr: 0.001
12-23 20:18:17 Epoch: 41 [192/1068], Train Loss: 0.2280 Train Acc: 0.9194,361.5 examples/sec 0.09 sec/batch
12-23 20:18:19 Epoch: 41 train-Loss: 0.1621 train-Acc: 0.9485, Cost 2.6618 sec
12-23 20:18:19 Epoch: 41 val-Loss: 0.5794 val-Acc: 0.8390, Cost 0.2725 sec
12-23 20:18:19 save best model epoch 41, acc 0.8390
12-23 20:18:19 -----Epoch 42/99-----
12-23 20:18:19 current lr: 0.001
12-23 20:18:22 Epoch: 42 train-Loss: 0.1285 train-Acc: 0.9522, Cost 2.7582 sec
12-23 20:18:22 Epoch: 42 val-Loss: 0.4555 val-Acc: 0.8876, Cost 0.2926 sec
12-23 20:18:22 save best model epoch 42, acc 0.8876
12-23 20:18:22 -----Epoch 43/99-----
12-23 20:18:22 current lr: 0.001
12-23 20:18:25 Epoch: 43 train-Loss: 0.1186 train-Acc: 0.9719, Cost 3.1977 sec
12-23 20:18:26 Epoch: 43 val-Loss: 0.5609 val-Acc: 0.8277, Cost 0.3531 sec
12-23 20:18:26 -----Epoch 44/99-----
12-23 20:18:26 current lr: 0.001
12-23 20:18:26 Epoch: 44 [128/1068], Train Loss: 0.1321 Train Acc: 0.9602,330.2 examples/sec 0.10 sec/batch
12-23 20:18:29 Epoch: 44 train-Loss: 0.0695 train-Acc: 0.9822, Cost 3.1875 sec
12-23 20:18:29 Epoch: 44 val-Loss: 0.4356 val-Acc: 0.8652, Cost 0.3934 sec
12-23 20:18:29 -----Epoch 45/99-----
12-23 20:18:29 current lr: 0.001
12-23 20:18:33 Epoch: 45 train-Loss: 0.0945 train-Acc: 0.9757, Cost 3.2820 sec
12-23 20:18:33 Epoch: 45 val-Loss: 0.5902 val-Acc: 0.8502, Cost 0.4549 sec
12-23 20:18:33 -----Epoch 46/99-----
12-23 20:18:33 current lr: 0.001
12-23 20:18:37 Epoch: 46 train-Loss: 0.0929 train-Acc: 0.9738, Cost 3.4349 sec
12-23 20:18:37 Epoch: 46 val-Loss: 0.6575 val-Acc: 0.8090, Cost 0.4044 sec
12-23 20:18:37 -----Epoch 47/99-----
12-23 20:18:37 current lr: 0.001
12-23 20:18:37 Epoch: 47 [64/1068], Train Loss: 0.0860 Train Acc: 0.9777,285.8 examples/sec 0.11 sec/batch
12-23 20:18:40 Epoch: 47 train-Loss: 0.0850 train-Acc: 0.9728, Cost 3.4011 sec
12-23 20:18:41 Epoch: 47 val-Loss: 0.7262 val-Acc: 0.8277, Cost 0.4501 sec
12-23 20:18:41 -----Epoch 48/99-----
12-23 20:18:41 current lr: 0.001
12-23 20:18:44 Epoch: 48 train-Loss: 0.0749 train-Acc: 0.9803, Cost 3.6087 sec
12-23 20:18:45 Epoch: 48 val-Loss: 0.5639 val-Acc: 0.8614, Cost 0.4587 sec
12-23 20:18:45 -----Epoch 49/99-----
12-23 20:18:45 current lr: 0.001
12-23 20:18:49 Epoch: 49 train-Loss: 0.0583 train-Acc: 0.9850, Cost 3.8125 sec
12-23 20:18:49 Epoch: 49 val-Loss: 0.4442 val-Acc: 0.8727, Cost 0.4763 sec
12-23 20:18:49 -----Epoch 50/99-----
12-23 20:18:49 current lr: 0.001
12-23 20:18:49 Epoch: 50 [0/1068], Train Loss: 0.0710 Train Acc: 0.9796,261.1 examples/sec 0.12 sec/batch
12-23 20:18:53 Epoch: 50 train-Loss: 0.0838 train-Acc: 0.9728, Cost 3.6164 sec
12-23 20:18:53 Epoch: 50 val-Loss: 0.8091 val-Acc: 0.8052, Cost 0.4895 sec
12-23 20:18:53 -----Epoch 51/99-----
12-23 20:18:53 current lr: 0.001
12-23 20:18:57 Epoch: 51 train-Loss: 0.0816 train-Acc: 0.9691, Cost 3.6793 sec
12-23 20:18:57 Epoch: 51 val-Loss: 0.5182 val-Acc: 0.8539, Cost 0.4303 sec
12-23 20:18:57 -----Epoch 52/99-----
12-23 20:18:57 current lr: 0.001
12-23 20:19:01 Epoch: 52 [1024/1068], Train Loss: 0.0715 Train Acc: 0.9759,268.7 examples/sec 0.12 sec/batch
12-23 20:19:01 Epoch: 52 train-Loss: 0.0462 train-Acc: 0.9869, Cost 3.7275 sec
12-23 20:19:02 Epoch: 52 val-Loss: 0.7428 val-Acc: 0.8315, Cost 0.5420 sec
12-23 20:19:02 -----Epoch 53/99-----
12-23 20:19:02 current lr: 0.001
12-23 20:19:05 Epoch: 53 train-Loss: 0.0659 train-Acc: 0.9794, Cost 3.6763 sec
12-23 20:19:06 Epoch: 53 val-Loss: 0.4774 val-Acc: 0.8689, Cost 0.4849 sec
12-23 20:19:06 -----Epoch 54/99-----
12-23 20:19:06 current lr: 0.001
12-23 20:19:10 Epoch: 54 train-Loss: 0.0436 train-Acc: 0.9860, Cost 3.6955 sec
12-23 20:19:10 Epoch: 54 val-Loss: 2.4037 val-Acc: 0.5955, Cost 0.4508 sec
12-23 20:19:10 -----Epoch 55/99-----
12-23 20:19:10 current lr: 0.001
12-23 20:19:13 Epoch: 55 [960/1068], Train Loss: 0.0602 Train Acc: 0.9831,257.7 examples/sec 0.12 sec/batch
12-23 20:19:14 Epoch: 55 train-Loss: 0.0727 train-Acc: 0.9831, Cost 3.5511 sec
12-23 20:19:14 Epoch: 55 val-Loss: 0.8581 val-Acc: 0.8090, Cost 0.4728 sec
12-23 20:19:14 -----Epoch 56/99-----
12-23 20:19:14 current lr: 0.001
12-23 20:19:18 Epoch: 56 train-Loss: 0.0650 train-Acc: 0.9794, Cost 3.6827 sec
12-23 20:19:18 Epoch: 56 val-Loss: 0.6851 val-Acc: 0.8539, Cost 0.4839 sec
12-23 20:19:18 -----Epoch 57/99-----
12-23 20:19:18 current lr: 0.001
12-23 20:19:22 Epoch: 57 train-Loss: 0.0747 train-Acc: 0.9822, Cost 3.8499 sec
12-23 20:19:23 Epoch: 57 val-Loss: 2.1114 val-Acc: 0.6217, Cost 0.4922 sec
12-23 20:19:23 -----Epoch 58/99-----
12-23 20:19:23 current lr: 0.001
12-23 20:19:26 Epoch: 58 [896/1068], Train Loss: 0.0589 Train Acc: 0.9834,255.7 examples/sec 0.12 sec/batch
12-23 20:19:26 Epoch: 58 train-Loss: 0.0406 train-Acc: 0.9878, Cost 3.5205 sec
12-23 20:19:27 Epoch: 58 val-Loss: 0.4948 val-Acc: 0.8914, Cost 0.5017 sec
12-23 20:19:27 save best model epoch 58, acc 0.8914
12-23 20:19:27 -----Epoch 59/99-----
12-23 20:19:27 current lr: 0.001
12-23 20:19:31 Epoch: 59 train-Loss: 0.0546 train-Acc: 0.9869, Cost 4.0732 sec
12-23 20:19:31 Epoch: 59 val-Loss: 0.6296 val-Acc: 0.8614, Cost 0.7322 sec
12-23 20:19:31 -----Epoch 60/99-----
12-23 20:19:31 current lr: 0.001
12-23 20:19:35 Epoch: 60 train-Loss: 0.0425 train-Acc: 0.9850, Cost 4.0708 sec
12-23 20:19:36 Epoch: 60 val-Loss: 0.5032 val-Acc: 0.8727, Cost 0.4928 sec
12-23 20:19:36 -----Epoch 61/99-----
12-23 20:19:36 current lr: 0.001
12-23 20:19:39 Epoch: 61 [832/1068], Train Loss: 0.0542 Train Acc: 0.9857,238.7 examples/sec 0.13 sec/batch
12-23 20:19:39 Epoch: 61 train-Loss: 0.0618 train-Acc: 0.9869, Cost 3.5014 sec
12-23 20:19:40 Epoch: 61 val-Loss: 0.6326 val-Acc: 0.8427, Cost 0.4969 sec
12-23 20:19:40 -----Epoch 62/99-----
12-23 20:19:40 current lr: 0.001
12-23 20:19:43 Epoch: 62 train-Loss: 0.0388 train-Acc: 0.9897, Cost 3.5145 sec
12-23 20:19:44 Epoch: 62 val-Loss: 0.8777 val-Acc: 0.7903, Cost 0.5149 sec
12-23 20:19:44 -----Epoch 63/99-----
12-23 20:19:44 current lr: 0.001
12-23 20:19:48 Epoch: 63 train-Loss: 0.0730 train-Acc: 0.9775, Cost 3.6681 sec
12-23 20:19:48 Epoch: 63 val-Loss: 1.1015 val-Acc: 0.7116, Cost 0.4979 sec
12-23 20:19:48 -----Epoch 64/99-----
12-23 20:19:48 current lr: 0.001
12-23 20:19:51 Epoch: 64 [768/1068], Train Loss: 0.0544 Train Acc: 0.9834,259.9 examples/sec 0.12 sec/batch
12-23 20:19:52 Epoch: 64 train-Loss: 0.0451 train-Acc: 0.9841, Cost 3.5800 sec
12-23 20:19:52 Epoch: 64 val-Loss: 1.2246 val-Acc: 0.7603, Cost 0.4896 sec
12-23 20:19:52 -----Epoch 65/99-----
12-23 20:19:52 current lr: 0.001
12-23 20:19:56 Epoch: 65 train-Loss: 0.0375 train-Acc: 0.9944, Cost 3.7114 sec
12-23 20:19:56 Epoch: 65 val-Loss: 0.3796 val-Acc: 0.8914, Cost 0.5125 sec
12-23 20:19:56 -----Epoch 66/99-----
12-23 20:19:56 current lr: 0.001
12-23 20:20:00 Epoch: 66 train-Loss: 0.0709 train-Acc: 0.9785, Cost 3.8867 sec
12-23 20:20:01 Epoch: 66 val-Loss: 0.6788 val-Acc: 0.8427, Cost 0.4962 sec
12-23 20:20:01 -----Epoch 67/99-----
12-23 20:20:01 current lr: 0.001
12-23 20:20:03 Epoch: 67 [704/1068], Train Loss: 0.0685 Train Acc: 0.9796,250.3 examples/sec 0.13 sec/batch
12-23 20:20:04 Epoch: 67 train-Loss: 0.1094 train-Acc: 0.9635, Cost 3.6201 sec
12-23 20:20:05 Epoch: 67 val-Loss: 0.5527 val-Acc: 0.8614, Cost 0.4525 sec
12-23 20:20:05 -----Epoch 68/99-----
12-23 20:20:05 current lr: 0.001
12-23 20:20:09 Epoch: 68 train-Loss: 0.0532 train-Acc: 0.9831, Cost 3.6394 sec
12-23 20:20:09 Epoch: 68 val-Loss: 0.4038 val-Acc: 0.9026, Cost 0.5340 sec
12-23 20:20:09 save best model epoch 68, acc 0.9026
12-23 20:20:09 -----Epoch 69/99-----
12-23 20:20:09 current lr: 0.001
12-23 20:20:13 Epoch: 69 train-Loss: 0.0401 train-Acc: 0.9916, Cost 3.6962 sec
12-23 20:20:13 Epoch: 69 val-Loss: 1.3272 val-Acc: 0.7228, Cost 0.4845 sec
12-23 20:20:13 -----Epoch 70/99-----
12-23 20:20:13 current lr: 0.001
12-23 20:20:16 Epoch: 70 [640/1068], Train Loss: 0.0500 Train Acc: 0.9873,256.7 examples/sec 0.12 sec/batch
12-23 20:20:17 Epoch: 70 train-Loss: 0.0402 train-Acc: 0.9888, Cost 3.6905 sec
12-23 20:20:17 Epoch: 70 val-Loss: 0.4582 val-Acc: 0.8801, Cost 0.5258 sec
12-23 20:20:17 -----Epoch 71/99-----
12-23 20:20:17 current lr: 0.001
12-23 20:20:21 Epoch: 71 train-Loss: 0.0235 train-Acc: 0.9934, Cost 3.7157 sec
12-23 20:20:22 Epoch: 71 val-Loss: 0.4065 val-Acc: 0.9026, Cost 0.5166 sec
12-23 20:20:22 -----Epoch 72/99-----
12-23 20:20:22 current lr: 0.001
12-23 20:20:25 Epoch: 72 train-Loss: 0.0367 train-Acc: 0.9934, Cost 3.6850 sec
12-23 20:20:26 Epoch: 72 val-Loss: 0.9524 val-Acc: 0.7828, Cost 0.5219 sec
12-23 20:20:26 -----Epoch 73/99-----
12-23 20:20:26 current lr: 0.001
12-23 20:20:28 Epoch: 73 [576/1068], Train Loss: 0.0494 Train Acc: 0.9860,252.5 examples/sec 0.12 sec/batch
12-23 20:20:30 Epoch: 73 train-Loss: 0.0950 train-Acc: 0.9710, Cost 3.7183 sec
12-23 20:20:30 Epoch: 73 val-Loss: 3.3979 val-Acc: 0.5206, Cost 0.4800 sec
12-23 20:20:30 -----Epoch 74/99-----
12-23 20:20:30 current lr: 0.001
12-23 20:20:34 Epoch: 74 train-Loss: 0.0357 train-Acc: 0.9888, Cost 3.8694 sec
12-23 20:20:34 Epoch: 74 val-Loss: 0.4726 val-Acc: 0.9176, Cost 0.4230 sec
12-23 20:20:34 save best model epoch 74, acc 0.9176
12-23 20:20:34 -----Epoch 75/99-----
12-23 20:20:34 current lr: 0.001
12-23 20:20:38 Epoch: 75 train-Loss: 0.0374 train-Acc: 0.9916, Cost 3.8537 sec
12-23 20:20:39 Epoch: 75 val-Loss: 0.3813 val-Acc: 0.9288, Cost 0.5528 sec
12-23 20:20:39 save best model epoch 75, acc 0.9288
12-23 20:20:39 -----Epoch 76/99-----
12-23 20:20:39 current lr: 0.001
12-23 20:20:41 Epoch: 76 [512/1068], Train Loss: 0.0369 Train Acc: 0.9898,245.9 examples/sec 0.13 sec/batch
12-23 20:20:43 Epoch: 76 train-Loss: 0.0270 train-Acc: 0.9916, Cost 3.8450 sec
12-23 20:20:43 Epoch: 76 val-Loss: 0.6101 val-Acc: 0.8727, Cost 0.5064 sec
12-23 20:20:43 -----Epoch 77/99-----
12-23 20:20:43 current lr: 0.001
12-23 20:20:47 Epoch: 77 train-Loss: 0.0211 train-Acc: 0.9953, Cost 3.6936 sec
12-23 20:20:47 Epoch: 77 val-Loss: 0.4052 val-Acc: 0.9139, Cost 0.5285 sec
12-23 20:20:47 -----Epoch 78/99-----
12-23 20:20:47 current lr: 0.001
12-23 20:20:51 Epoch: 78 train-Loss: 0.0260 train-Acc: 0.9944, Cost 3.8357 sec
12-23 20:20:52 Epoch: 78 val-Loss: 0.4523 val-Acc: 0.8951, Cost 0.5173 sec
12-23 20:20:52 -----Epoch 79/99-----
12-23 20:20:52 current lr: 0.001
12-23 20:20:53 Epoch: 79 [448/1068], Train Loss: 0.0243 Train Acc: 0.9939,248.0 examples/sec 0.13 sec/batch
12-23 20:20:56 Epoch: 79 train-Loss: 0.0221 train-Acc: 0.9916, Cost 3.8037 sec
12-23 20:20:56 Epoch: 79 val-Loss: 0.4195 val-Acc: 0.8989, Cost 0.5467 sec
12-23 20:20:56 -----Epoch 80/99-----
12-23 20:20:56 current lr: 0.001
12-23 20:21:00 Epoch: 80 train-Loss: 0.0417 train-Acc: 0.9906, Cost 3.9510 sec
12-23 20:21:01 Epoch: 80 val-Loss: 0.5067 val-Acc: 0.8951, Cost 0.6233 sec
12-23 20:21:01 -----Epoch 81/99-----
12-23 20:21:01 current lr: 0.001
12-23 20:21:03 Epoch: 81 train-Loss: 0.0242 train-Acc: 0.9934, Cost 2.8125 sec
12-23 20:21:04 Epoch: 81 val-Loss: 0.8954 val-Acc: 0.8240, Cost 0.3469 sec
12-23 20:21:04 -----Epoch 82/99-----
12-23 20:21:04 current lr: 0.001
12-23 20:21:05 Epoch: 82 [384/1068], Train Loss: 0.0277 Train Acc: 0.9920,276.5 examples/sec 0.11 sec/batch
12-23 20:21:06 Epoch: 82 train-Loss: 0.0266 train-Acc: 0.9925, Cost 2.6141 sec
12-23 20:21:07 Epoch: 82 val-Loss: 0.4733 val-Acc: 0.9176, Cost 0.3622 sec
12-23 20:21:07 -----Epoch 83/99-----
12-23 20:21:07 current lr: 0.001
12-23 20:21:09 Epoch: 83 train-Loss: 0.0267 train-Acc: 0.9934, Cost 2.5998 sec
12-23 20:21:10 Epoch: 83 val-Loss: 0.4302 val-Acc: 0.9101, Cost 0.3691 sec
12-23 20:21:10 -----Epoch 84/99-----
12-23 20:21:10 current lr: 0.001
12-23 20:21:12 Epoch: 84 train-Loss: 0.0204 train-Acc: 0.9925, Cost 2.7335 sec
12-23 20:21:13 Epoch: 84 val-Loss: 1.2109 val-Acc: 0.7416, Cost 0.3415 sec
12-23 20:21:13 -----Epoch 85/99-----
12-23 20:21:13 current lr: 0.001
12-23 20:21:14 Epoch: 85 [320/1068], Train Loss: 0.0268 Train Acc: 0.9924,352.7 examples/sec 0.09 sec/batch
12-23 20:21:15 Epoch: 85 train-Loss: 0.0256 train-Acc: 0.9934, Cost 2.6108 sec
12-23 20:21:16 Epoch: 85 val-Loss: 0.5003 val-Acc: 0.8989, Cost 0.3512 sec
12-23 20:21:16 -----Epoch 86/99-----
12-23 20:21:16 current lr: 0.001
12-23 20:21:18 Epoch: 86 train-Loss: 0.0250 train-Acc: 0.9944, Cost 2.7038 sec
12-23 20:21:19 Epoch: 86 val-Loss: 0.5542 val-Acc: 0.8764, Cost 0.3407 sec
12-23 20:21:19 -----Epoch 87/99-----
12-23 20:21:19 current lr: 0.001
12-23 20:21:21 Epoch: 87 train-Loss: 0.0324 train-Acc: 0.9869, Cost 2.6647 sec
12-23 20:21:22 Epoch: 87 val-Loss: 0.8284 val-Acc: 0.8052, Cost 0.3717 sec
12-23 20:21:22 -----Epoch 88/99-----
12-23 20:21:22 current lr: 0.001
12-23 20:21:23 Epoch: 88 [256/1068], Train Loss: 0.0267 Train Acc: 0.9917,352.8 examples/sec 0.09 sec/batch
12-23 20:21:25 Epoch: 88 train-Loss: 0.0151 train-Acc: 0.9944, Cost 2.7030 sec
12-23 20:21:25 Epoch: 88 val-Loss: 0.5691 val-Acc: 0.8801, Cost 0.3561 sec
12-23 20:21:25 -----Epoch 89/99-----
12-23 20:21:25 current lr: 0.001
12-23 20:21:28 Epoch: 89 train-Loss: 0.0178 train-Acc: 0.9944, Cost 2.5830 sec
12-23 20:21:28 Epoch: 89 val-Loss: 0.4885 val-Acc: 0.9139, Cost 0.3715 sec
12-23 20:21:28 -----Epoch 90/99-----
12-23 20:21:28 current lr: 0.001
12-23 20:21:31 Epoch: 90 train-Loss: 0.0189 train-Acc: 0.9916, Cost 2.6558 sec
12-23 20:21:31 Epoch: 90 val-Loss: 0.5993 val-Acc: 0.8689, Cost 0.3534 sec
12-23 20:21:31 -----Epoch 91/99-----
12-23 20:21:31 current lr: 0.001
12-23 20:21:31 Epoch: 91 [192/1068], Train Loss: 0.0167 Train Acc: 0.9936,353.0 examples/sec 0.09 sec/batch
12-23 20:21:34 Epoch: 91 train-Loss: 0.0129 train-Acc: 0.9963, Cost 2.6535 sec
12-23 20:21:34 Epoch: 91 val-Loss: 0.5228 val-Acc: 0.8914, Cost 0.3922 sec
12-23 20:21:34 -----Epoch 92/99-----
12-23 20:21:34 current lr: 0.001
12-23 20:21:37 Epoch: 92 train-Loss: 0.0205 train-Acc: 0.9953, Cost 2.7449 sec
12-23 20:21:37 Epoch: 92 val-Loss: 0.7095 val-Acc: 0.8464, Cost 0.3576 sec
12-23 20:21:37 -----Epoch 93/99-----
12-23 20:21:37 current lr: 0.001
12-23 20:21:40 Epoch: 93 train-Loss: 0.0142 train-Acc: 0.9963, Cost 2.8241 sec
12-23 20:21:40 Epoch: 93 val-Loss: 0.7194 val-Acc: 0.8464, Cost 0.3260 sec
12-23 20:21:40 -----Epoch 94/99-----
12-23 20:21:40 current lr: 0.001
12-23 20:21:41 Epoch: 94 [128/1068], Train Loss: 0.0166 Train Acc: 0.9952,344.2 examples/sec 0.09 sec/batch
12-23 20:21:43 Epoch: 94 train-Loss: 0.0210 train-Acc: 0.9925, Cost 2.7434 sec
12-23 20:21:43 Epoch: 94 val-Loss: 0.6099 val-Acc: 0.8876, Cost 0.3586 sec
12-23 20:21:43 -----Epoch 95/99-----
12-23 20:21:43 current lr: 0.001
12-23 20:21:46 Epoch: 95 train-Loss: 0.0303 train-Acc: 0.9925, Cost 2.7537 sec
12-23 20:21:46 Epoch: 95 val-Loss: 0.4793 val-Acc: 0.9026, Cost 0.3653 sec
12-23 20:21:46 -----Epoch 96/99-----
12-23 20:21:46 current lr: 0.001
12-23 20:21:49 Epoch: 96 train-Loss: 0.0738 train-Acc: 0.9803, Cost 2.7527 sec
12-23 20:21:50 Epoch: 96 val-Loss: 9.2191 val-Acc: 0.4007, Cost 0.3900 sec
12-23 20:21:50 -----Epoch 97/99-----
12-23 20:21:50 current lr: 0.001
12-23 20:21:50 Epoch: 97 [64/1068], Train Loss: 0.0475 Train Acc: 0.9876,341.3 examples/sec 0.09 sec/batch
12-23 20:21:52 Epoch: 97 train-Loss: 0.0909 train-Acc: 0.9757, Cost 2.7641 sec
12-23 20:21:53 Epoch: 97 val-Loss: 0.6851 val-Acc: 0.8502, Cost 0.4125 sec
12-23 20:21:53 -----Epoch 98/99-----
12-23 20:21:53 current lr: 0.001
12-23 20:21:56 Epoch: 98 train-Loss: 0.0558 train-Acc: 0.9813, Cost 2.7780 sec
12-23 20:21:56 Epoch: 98 val-Loss: 0.4663 val-Acc: 0.8914, Cost 0.3709 sec
12-23 20:21:56 -----Epoch 99/99-----
12-23 20:21:56 current lr: 0.001
12-23 20:21:59 Epoch: 99 train-Loss: 0.0345 train-Acc: 0.9897, Cost 2.7643 sec
12-23 20:21:59 Epoch: 99 val-Loss: 0.5272 val-Acc: 0.8876, Cost 0.3376 sec
12-23 20:21:59 save best model epoch 99, acc 0.8876
